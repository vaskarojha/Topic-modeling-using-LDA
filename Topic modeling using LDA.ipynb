{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4962133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ea5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.read_csv('data/papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f8f324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f924e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'year', 'title', 'event_type', 'pdf_name', 'abstract',\n",
       "       'paper_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "709fbeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = papers.drop(columns = ['id', 'event_type', 'pdf_name'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fa9791c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7236</th>\n",
       "      <td>1994</td>\n",
       "      <td>Single Transistor Learning Synapses</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Single Transistor Learning Synapses\\n\\nPaul Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7237</th>\n",
       "      <td>1994</td>\n",
       "      <td>Bias, Variance and the Combination of Least Sq...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bias, Variance and the Combination of\\nLeast S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7238</th>\n",
       "      <td>1994</td>\n",
       "      <td>A Real Time Clustering CMOS Neural Engine</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>A Real Time Clustering CMOS\\nNeural Engine\\nT....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7239</th>\n",
       "      <td>1994</td>\n",
       "      <td>Learning direction in global motion: two class...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Learning direction in global motion: two\\nclas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7240</th>\n",
       "      <td>1994</td>\n",
       "      <td>Correlation and Interpolation Networks for Rea...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Correlation and Interpolation Networks for\\nRe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7241 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                                              title  \\\n",
       "0     1987  Self-Organization of Associative Database and ...   \n",
       "1     1987  A Mean Field Theory of Layer IV of Visual Cort...   \n",
       "2     1988  Storing Covariance by the Associative Long-Ter...   \n",
       "3     1994  Bayesian Query Construction for Neural Network...   \n",
       "4     1994  Neural Network Ensembles, Cross Validation, an...   \n",
       "...    ...                                                ...   \n",
       "7236  1994                Single Transistor Learning Synapses   \n",
       "7237  1994  Bias, Variance and the Combination of Least Sq...   \n",
       "7238  1994          A Real Time Clustering CMOS Neural Engine   \n",
       "7239  1994  Learning direction in global motion: two class...   \n",
       "7240  1994  Correlation and Interpolation Networks for Rea...   \n",
       "\n",
       "              abstract                                         paper_text  \n",
       "0     Abstract Missing  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1     Abstract Missing  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2     Abstract Missing  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3     Abstract Missing  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4     Abstract Missing  Neural Network Ensembles, Cross\\nValidation, a...  \n",
       "...                ...                                                ...  \n",
       "7236  Abstract Missing  Single Transistor Learning Synapses\\n\\nPaul Ha...  \n",
       "7237  Abstract Missing  Bias, Variance and the Combination of\\nLeast S...  \n",
       "7238  Abstract Missing  A Real Time Clustering CMOS\\nNeural Engine\\nT....  \n",
       "7239  Abstract Missing  Learning direction in global motion: two\\nclas...  \n",
       "7240  Abstract Missing  Correlation and Interpolation Networks for\\nRe...  \n",
       "\n",
       "[7241 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53b4a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = papers.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da95146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>2000</td>\n",
       "      <td>Sparse Representation for Gaussian Process Models</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Sparse Representation for Gaussian Process\\nMo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>2005</td>\n",
       "      <td>Unbiased Estimator of Shape Parameter for Spik...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Unbiased Estimator of Shape Parameter for\\nSpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>2016</td>\n",
       "      <td>Weight Normalization: A Simple Reparameterizat...</td>\n",
       "      <td>We present weight normalization: a reparameter...</td>\n",
       "      <td>Weight Normalization: A Simple Reparameterizat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>2007</td>\n",
       "      <td>Optimal ROC Curve for a Combination of Classif...</td>\n",
       "      <td>We present a new analysis for the combination ...</td>\n",
       "      <td>Optimal ROC Curve for a Combination of Classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>2004</td>\n",
       "      <td>Newscast EM</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Newscast EM\\nWojtek Kowalczyk\\nDepartment of C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>2014</td>\n",
       "      <td>Analysis of Brain States from Multi-Region LFP...</td>\n",
       "      <td>The local field potential (LFP) is a source of...</td>\n",
       "      <td>Analysis of Brain States\\nfrom Multi-Region LF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2172</th>\n",
       "      <td>2006</td>\n",
       "      <td>Learning from Multiple Sources</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Learning from Multiple Sources\\n\\nKoby Crammer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>1997</td>\n",
       "      <td>Structural Risk Minimization for Nonparametric...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Structural Risk Minimization for\\nNonparametri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3902</th>\n",
       "      <td>2012</td>\n",
       "      <td>Learning to Discover Social Circles in Ego Net...</td>\n",
       "      <td>Our personal social networks are big and clutt...</td>\n",
       "      <td>Learning to Discover Social Circles in Ego Net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5517</th>\n",
       "      <td>2015</td>\n",
       "      <td>Online F-Measure Optimization</td>\n",
       "      <td>The F-measure is an important and commonly use...</td>\n",
       "      <td>Online F-Measure Optimization\\nR?obert Busa-Fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                                              title  \\\n",
       "976   2000  Sparse Representation for Gaussian Process Models   \n",
       "2088  2005  Unbiased Estimator of Shape Parameter for Spik...   \n",
       "5652  2016  Weight Normalization: A Simple Reparameterizat...   \n",
       "2495  2007  Optimal ROC Curve for a Combination of Classif...   \n",
       "1817  2004                                        Newscast EM   \n",
       "...    ...                                                ...   \n",
       "5108  2014  Analysis of Brain States from Multi-Region LFP...   \n",
       "2172  2006                     Learning from Multiple Sources   \n",
       "519   1997  Structural Risk Minimization for Nonparametric...   \n",
       "3902  2012  Learning to Discover Social Circles in Ego Net...   \n",
       "5517  2015                      Online F-Measure Optimization   \n",
       "\n",
       "                                               abstract  \\\n",
       "976                                    Abstract Missing   \n",
       "2088                                   Abstract Missing   \n",
       "5652  We present weight normalization: a reparameter...   \n",
       "2495  We present a new analysis for the combination ...   \n",
       "1817                                   Abstract Missing   \n",
       "...                                                 ...   \n",
       "5108  The local field potential (LFP) is a source of...   \n",
       "2172                                   Abstract Missing   \n",
       "519                                    Abstract Missing   \n",
       "3902  Our personal social networks are big and clutt...   \n",
       "5517  The F-measure is an important and commonly use...   \n",
       "\n",
       "                                             paper_text  \n",
       "976   Sparse Representation for Gaussian Process\\nMo...  \n",
       "2088  Unbiased Estimator of Shape Parameter for\\nSpi...  \n",
       "5652  Weight Normalization: A Simple Reparameterizat...  \n",
       "2495  Optimal ROC Curve for a Combination of Classif...  \n",
       "1817  Newscast EM\\nWojtek Kowalczyk\\nDepartment of C...  \n",
       "...                                                 ...  \n",
       "5108  Analysis of Brain States\\nfrom Multi-Region LF...  \n",
       "2172  Learning from Multiple Sources\\n\\nKoby Crammer...  \n",
       "519   Structural Risk Minimization for\\nNonparametri...  \n",
       "3902  Learning to Discover Social Circles in Ego Net...  \n",
       "5517  Online F-Measure Optimization\\nR?obert Busa-Fe...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a27b7",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25d67e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # removing punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d260e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b748c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers['clean_text']= papers['paper_text'].map(lambda x: re.sub('[,\\.!?]','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d24e08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976     Sparse Representation for Gaussian Process\\nMo...\n",
       "2088    Unbiased Estimator of Shape Parameter for\\nSpi...\n",
       "5652    Weight Normalization: A Simple Reparameterizat...\n",
       "2495    Optimal ROC Curve for a Combination of Classif...\n",
       "1817    Newscast EM\\nWojtek Kowalczyk\\nDepartment of C...\n",
       "                              ...                        \n",
       "5108    Analysis of Brain States\\nfrom Multi-Region LF...\n",
       "2172    Learning from Multiple Sources\\n\\nKoby Crammer...\n",
       "519     Structural Risk Minimization for\\nNonparametri...\n",
       "3902    Learning to Discover Social Circles in Ego Net...\n",
       "5517    Online F-Measure Optimization\\nRobert Busa-Fek...\n",
       "Name: clean_text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdfeeee",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6393d0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13524/3266101202.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089680bf",
   "metadata": {},
   "source": [
    "## document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0927cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def create_document_term_matrix(dataframe , column_name):\n",
    "    cv = CountVectorizer(analyzer = 'word')\n",
    "    data = cv.fit_transform(dataframe[column_name])\n",
    "    df_dtm = pd.DataFrame(data.toarray(), columns= cv.get_feature_names_out())\n",
    "    df_dtm.index = dataframe.index\n",
    "    return df_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28f46567",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dtm = create_document_term_matrix(papers, 'clean_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23c1b643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>000000000</th>\n",
       "      <th>00001</th>\n",
       "      <th>00003</th>\n",
       "      <th>0001</th>\n",
       "      <th>0001010000</th>\n",
       "      <th>00014</th>\n",
       "      <th>000181</th>\n",
       "      <th>...</th>\n",
       "      <th>zux1</th>\n",
       "      <th>zw</th>\n",
       "      <th>zw1</th>\n",
       "      <th>zweig</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zwols</th>\n",
       "      <th>zx</th>\n",
       "      <th>zy</th>\n",
       "      <th>zyi</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6574</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7104</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4699</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6968</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6018</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5344</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 20534 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  0000  000000000  00001  00003  0001  0001010000  00014  000181  \\\n",
       "6574   0    0     0          0      0      0     0           0      0       0   \n",
       "2044   0    0     0          0      0      0     0           0      0       0   \n",
       "7104   0    0     0          0      0      0     0           0      0       0   \n",
       "4699   0    0     0          0      0      0     0           0      0       0   \n",
       "1759   0    0     0          0      0      0     0           0      0       0   \n",
       "6968   1    0     0          0      0      0     0           0      0       0   \n",
       "1263   0    0     0          0      0      0     0           0      0       0   \n",
       "2261   0    0     0          0      0      0     0           0      0       0   \n",
       "6018   1    1     0          0      0      0     0           0      0       0   \n",
       "5344   0    0     0          0      0      0     0           0      0       0   \n",
       "\n",
       "      ...  zux1  zw  zw1  zweig  zwick  zwols  zx  zy  zyi  zz  \n",
       "6574  ...     0   0    0      0      0      0   0   0    0   0  \n",
       "2044  ...     0   0    0      0      0      0   0   0    0   0  \n",
       "7104  ...     0   0    0      0      0      0   0   0    0   0  \n",
       "4699  ...     0   0    0      0      0      0   0   0    0   0  \n",
       "1759  ...     0   0    0      0      0      0   0   0    0   0  \n",
       "6968  ...     0   0    0      0      0      0   0   0    0   0  \n",
       "1263  ...     0   0    0      0      0      0   0   0    0   0  \n",
       "2261  ...     0   0    0      0      0      0   0   0    0   0  \n",
       "6018  ...     0   0    0      0      0      0   0   0    0   0  \n",
       "5344  ...     0   0    0      0      0      0   0   0    0   0  \n",
       "\n",
       "[10 rows x 20534 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dtm.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3853c35",
   "metadata": {},
   "source": [
    "## data modeling and tokeninzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9906494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cb5b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fbd8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53e5edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7f2501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d813135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8dce37e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0f6e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add more words to the list\n",
    "stop_words.extend(['has', 'been', 're', 'com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cdd08db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence_to_word(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc = True))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01601706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[ word for word in simple_preprocess(str(doc))\n",
    "            if word not in stop_words] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f838f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_list = papers.clean_text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e8b172f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08daa98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sparse Representation for Gaussian Process\\nModels\\n\\nLehel Csat6 and Manfred Opper\\nNeural Computing Research Group\\nSchool of Engineering and Applied Sciences\\nB4 7ET Birmingham United Kingdom\\n{csat o l oppe r m} @as t o n ac uk\\n\\nAbstract\\nWe develop an approach for a sparse representation for Gaussian Process\\n(GP) models in order to overcome the limitations of GPs caused by large\\ndata sets The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample\\nof the data which fully specifies the prediction of the model Experimental results on toy examples and large real-world data sets indicate the\\nefficiency of the approach\\n\\n1 Introduction\\nGaussian processes (GP) [1; 15] provide promising non-parametric tools for modelling\\nreal-world statistical problems Like other kernel based methods eg Support Vector Machines (SVMs) [13] they combine a high flexibility ofthe model by working in high (often\\n00) dimensional feature spaces with the simplicity that all operations are \"kernelized\" ie\\nthey are performed in the (lower dimensional) input space using positive definite kernels\\nAn important advantage of GPs over other non-Bayesian models is the explicit probabilistic\\nformulation of the model This does not only provide the modeller with (Bayesian) confidence intervals (for regression) or posterior class probabilities (for classification) but also\\nimmediately opens the possibility to treat other nonstandard data models (eg Quantum\\ninverse statistics [4])\\nUnfortunately the drawback of GP models (which was originally apparent in SVMs as well\\nbut has now been overcome [6]) lies in the huge increase of the computational cost with\\nthe number of training data This seems to preclude applications of GPs to large datasets\\nThis paper presents an approach to overcome this problem It is based on a combination of\\nan online learning approach requiring only a single sweep through the data and a method\\nto reduce the number of parameters representing the model\\nMaking use of the proposed parametrisation the method extracts a subset of the examples\\nand the prediction relies only on these basis vectors (BV) The memory requirement of the\\nalgorithm scales thus only with the size of this set Experiments with real-world datasets\\nconfirm the good performance of the proposed method 1\\n1A\\n\\ndifferent approach for dealing with large datasets was suggested by V Tresp [12] His method\\n\\n\\x0c2\\n\\nGaussian Process Models\\n\\nGPs belong to Bayesian non-parametric models where likelihoods are parametrised by a\\nGaussian stochastic process (random field) a(x) which is indexed by the continuous input\\nvariable x  The prior knowledge about a is expressed in the prior mean and the covariance\\ngiven by the kernel Ko(xx\\') = Cov(a(x) a(x\\')) [14; 15] In the following only zero\\nmean GP priors are used\\nIn supervised learning the process a(x) is used as a latent variable in the likelihood\\nP(yla(x)) which denotes the probability of output Y given the input x  Based on a set\\nof input-output pairs (xn Yn) with Xn E R m and Yn E R (n = 1 N) the Bayesian learning method computes the posterior distribution of the process a(x) using the prior and\\nlikelihood [14; 15; 3]\\nAlthough the prior is a Gaussian process the posterior process usually is not Gaussian\\n(except for the special case of regression with Gaussian noise) Nevertheless various approaches have been introduced recently to approximate the posterior averages [11 ; 9] Our\\napproach is based on the idea of approximating the true posterior process p{ a} by a Gaussian process q{a} which is fully specified by a covariance kernel Kt(xx\\') and posterior\\nmean (a(x))t where t is the number of training data processed by the algorithm so far\\nSuch an approximation could be formulated within the variational approach where q is\\nchosen such that the relative entropy D(qp) == Eq In ~ is minimal [9] However in this\\nformulation the expectation is over the approximate process q rather than over p It seems\\nintuitively better to minimise the other KL divergence given by D(p q) == Ep In ~ because the expectation is over the true distribution Unfortunately such a computation is\\ngenerally not possible The following online approach can be understood as an approximation to this task\\n\\n3\\n\\nOnline learning for Gaussian Processes\\n\\nIn this section we briefly review the main idea of the Bayesian online approach (see eg [5])\\nto GP models We process the training data sequentially one after the other Assume we\\nhave a Gaussian approximation to the posterior process at time t We use the next example\\nt + 1 to update the posterior using Bayes rule via\\n\\np(a) = P(Yt+1la(Xt+l))Pt(q)\\n(P(Yt+1la(xt+1)))t\\nSince the resulting posterior p(q) is non-Gaussian we project it to the closest Gaussian\\nprocess q which minimises the KL divergence D(p q) Note that now the new approximation q is on \"correct\" side of the KL divergence The minimisation can be performed\\nexactly leading to a match of the means and covariances of p and q Since p is much less\\ncomplex than the full posterior it is possible to write down the changes in the first two\\nmoments analytically [2]:\\n\\n(a(x))t+1 = (a(x))t + b1 Kt(xxt+d\\nK t+1(xx\\') = Kt(xx\\') + b2 K t (xxt+1)Kt (xt+1x\\')\\n\\n(1)\\n\\nwhere the scalar coefficients b1 and b2 are:\\n(2)\\n\\nwith averaging performed with respect to the marginal Gaussian distribution of the process\\nvariable a at input Xt+1\\' Note that this yields a one dimensional integral Derivatives are\\nis based on splitting the data-set into smaller subsets and training individual GP predictors on each of\\nthem The final prediction is achieved by a specific weighting of the individual predictors\\n\\n\\x0c<PH\\n\\n~\\'\\n\\n\\n\\n/:\\'~es\\n\\n-------(a)\\n\\n-\\n\\n(b)\\n\\nFigure 1: Projection of the new input <Pt+ to the subspace spanned by previous inputs\\n<l>t+l is the projection to the linear span of {<Pih=l t and <Pres the residual vector Subfigure (a) shows the projections to the subspace and (b) gives a geometric picture of the\\n\"measurable part\" of the error It+ from eq (8)\\ntaken with respect to (a(x))t  Note also that this procedure does not equal the extended\\nKalman filter which involves linearisations of likelihoods whereas in our approach it is\\npossible to use non-smooth likelihoods (eg noise free classifications) without problems\\nIt turns out that the recursion (1) is solved by the parametrisation\\n\\n(a(x))t\\nKt(xx\\')\\n\\n=\\n=\\n\\nL~=IKo(xxi)at(i)\\nKo(xx\\')\\n\\n+ LL=IKo(xXi)Ct(ij)Ko(xjx\\')\\n\\n(3)\\n\\nsuch that in each on-line step we have to update only the vector of a\\'s and the matrix\\nof C\\'s For notational convenience we use vector at = [at(1)   at (N)jT and matrix\\nC t = {Ct (ij) hj=IN Zero-mean GP with kernel Ko is used as the starting point for the\\nalgorithm: ao = a and Co = a will be the starting parameters\\nThe update of the parameters defined in (3) is found to be\\n\\nat+\\n\\n= at + bl [Ctkt+l + et+l\\n\\n(4)\\n\\nC t+ = C t + b2 [C tkt+l + et+l [C tkt+ + et+lf\\nwith kt+ = [KO(XIXt+)   Ko(xt xt+)jT et+ the t + 1-th unit vector (all components except t + 1-th are zero) and the scalar coefficients bl and b2 computed from (2)\\nThe serious drawback of this approach which it shares with many other kernel methods is\\nthe quadratic increase of the matrix size with the training data\\n\\n4\\n\\nSparse representation\\n\\nWe use the following idea for reducing the increase of the size of C and a (for a similar approach see [8]) We consider the feature expansion of the kernel Ko(xx\\') = <p(X)T <p(x\\')\\nand decompose the new feature vector <p(Xt+) as a linear combination of the previous\\nfeatures and a residual <Pres:\\n\\n<p(Xt+)\\n\\n= <Pt+ + <Pres = \"t\\n~ i=l ei<p(Xi) + <Pres\\nA\\n\\nA\\n\\n(5)\\n\\nwhere <l>t+ is the projection of <Pt+ to the previous inputs and et+ = [el\\'    \\' etjT are\\nthe coordinates of <l>t+ with respect to the basis {<Pih=lt We can then re-express the GP\\nmeans:\\n\\n(6)\\n\\n\\x0cwith Qt+l(i) = at+l(i) + et+1(i)at+1(t + 1) and \\'YHI the residual (or novelty factor)\\nassociated with the new feature vector The vector et+1 and the residual term \\'Yt+1 are all\\nexpressed in terms of kernels:\\n(7)\\net+1 - K(-I)k\\nB\\nHI\\n\\'Yt+1 -- k*t+1 - kT\\nt+1 K(-I)k\\nB\\nt+1\\nwith KB(ij) = {KO(XiXj)hj=lt and kt+1 = K o (Xt+1Xt+1) The relation between\\nthe quantities et+1 and \\'Yt+1 is illustrated in Figure 1\\nA\\n\\n_\\n\\nNeglecting the last term in the decomposition of the new input (5) and performing the\\nupdate with the resulting vector is equivalent to the update rule (4) with et+1 replaced by\\net+1 Note that the dimension of parameter space is not increased by this approximative\\nupdate The memory required by the algorithm scales quadratically only with the size of\\nthe set of \"basis vectors\" ie those examples for which the full update (4) is made This\\nis similar to Support Vectors [13] without the need to solve the (high dimensional) convex\\noptimisation problem It is also related to the kernel PCA and the reduced set method [8]\\nwhere the full solution is computed first and then a reduced set is used for prediction\\nReplacing the input vector cJl t +1 by its projection on the linear span of the BVs when\\nupdating the GP parameters induces changes in the GP2 However the replacement of the\\ntrue feature vector by its approximation leaves the mean function unchanged at each BV i =\\n1 t That is the functions (a(x))t+1 from (6) and (a(x))t+1 = L~=I Qt+1(i)Ko (XiX)\\nhave the same value at all Xl The change at Xt+1 is\\nCt+1\\n\\n= l(a(xt+1))t+1 -\\n\\n(a(xt+t})t+11\\n\\n= Ibl l\\'Yt+1\\n\\n(8)\\n\\nwith bi the factor from (2)\\nAs a consequence a good approximation to the full GP solution is obtained if the input\\nfor which we have only a small change in the mean function of the posterior process is not\\nincluded in the set of BV s The change is given by Ct+1 and the decision of including Xt+1\\nor not is based on the \"score\" associated to it\\nThe absence of matrix inversions is an important issue when dealing with large datasets\\nThe matrix inversion from the projection equation (7) can be avoided by iterative inversion 3\\nof the Gram matrix Q = Ki/:\\n\\nQ t+1\\n\\n= Qt + \\'Yt;1 (et+1 -\\n\\net+t) (et+1 - et+If\\n\\n(9)\\n\\nAn important comment is that if the new input is in the linear span of the BVs then it will\\nnot be included in the basis set avoiding thus: 1) the small singular values of the matrix\\nK Band 2) the redundancy in representing the problem\\n\\n41\\n\\nDeleting a basis vector\\n\\nThe above section gave a method to leave out a vector that is not significant for the prediction purposes However it did not provide us with a method to eliminate one of the already\\nexisting BV-s\\nLet us assume that an input Xt+1 has just been added to the set of BV s Since we know that\\nan addition had taken place the update rule (4) with the t + 1-th unit vector et+1 was last\\nperformed Since the model parameters at the previous step had an empty t + 1-th row and\\ncolumn the parameters before thefull update can be identified\\nThe removal of the last basis vector can be done with the following steps: 1) computing\\nthe parameters before the update of the GP and 2) performing a reduced update of the\\n2Equation (7) also minimises the KL-distance between the full posterior (the one that increases\\nparameter space) and a parametric distribution using only the old BVs\\n3 A guide is available from Sam Roweis: http://wwwgatsbyuclacuk/rvroweis/noteshtml\\n\\n\\x0cQt+l\\n\\nC t+ l\\n\\nd t)\\n\\ndt )\\n\\nc*\\n\\nQ\\n\\n  \\n\\nC* T\\n\\nQ *T\\n\\nc*\\n\\nq*\\n\\nFigure 2: Decomposition of model parameters for the update equation (10)\\nmodel without the inclusion of the basis vector (eq (4) using et+1)\\' The updates for model\\nparameters a C and Q are \"inverted\" by inverting the coupled equations (4) and (9):\\n\\nQ*\\n& = a(t) - a*q*\\n\\nC=\\n\\nC(t)\\n\\n+ c* Q*Q*T\\n\\n_\\n\\nq*2\\n*Q*T\\nQ=Q(t) _Q__\\n\\n~\\nq*\\n\\n[Q*C*T\\n\\n+ C*Q*T]\\n\\n(10)\\n\\nq*\\nwhere the elements needed to update the model are extracted from the extended parameters\\nas illustrated in Figure 2\\n\\nThe consequence of the identification permits us to evaluate the score for the last BV But\\nsince the order of the BVs is approximately arbitrary we can assign a score to each BV\\n\\nlat+l(i)1\\nCi\\n\\n(11)\\n\\n= Qt+1 (i i)\\'\\n\\nThus we have a method to estimate the score of each basis vector at any time and to eliminate the one with the least contribution to the GP output (the mean) providing a sparse GP\\nwith a full control over memory size\\n\\n5 Simulation results\\nTo apply the online learning rules (4) the data likelihood for the specific problem has to be\\naveraged with respect to a Gaussian Using eq (2) the coefficients b1 and b2 are obtained\\nThe marginal of the GP at Xt+1 is a normal distribution with mean (a(Xt+1))t = a[k t+1\\nand variance 0\\';\\'+1 = kt+1 +k;+1 C tkt+1 where the GP parameters at time t are considered\\nAs a first example we consider regression with Gaussian output noise 0\\'5 for which\\n\\n1 (\\n2\\n2\\n)\\n(Yt+1 - (a(Xt+1)t)2\\n(12)\\nIn(P(Yt+1l a(Xt+d))t=-2\"ln 271\\'(0\\'0+0\\'11:\\'+1) 2( 2+ 2 )2\\n0\\'0 0\\'11:\\'+1\\nFor classification we use the probit model The outputs are binary Y E {-I I} and the\\nprobability is given by the error function (where u = ya/O\\'o):\\n1\\nP(yla) = Erf ( -ya) =  f(C\\n0\\'0\\nV 271\\'\\n\\nl\\n\\nU\\n\\ndte- t2 / 2\\n\\n00\\n\\nThe averaged log-likelihood for the new data point at time tis:\\n\\n(P (Yt+1 Ia (Xt+1 ))) =\\n\\nErf\\n\\n( Yt+1\\n\\na[k t+1 )\\n\\nj 0\\'5 + O\\'i\\'+l\\n\\n(13)\\n\\n\\x0c14\\n\\n~\\n\\n12\\n\\n\"\\n08\\n06\\n04\\n02\\n\\n~\\n\\n-02\\n\\n~\\n- 04\\n-3\\n\\n-2\\n\\n-1\\n\\n100\\n\\n150\\n\\n(a)\\n\\n200\\n\\n250 300 350 400\\n# of BasIs Vectors\\n\\n450\\n\\n500\\n\\n550\\n\\n(b)\\n\\nFigure 3: Simulation results for regression (a) and classification (b) For details see text\\n\\nFor the regression case we have chosen the toy data model y = sin(x)/x + ( where ( is\\na zero-mean Gaussian random variable with variance (]\"~ and an RBF kernel Figure 3a\\nshows the result of applying the algorithm for 600 input data and restricting the number\\nof BVs to 20 The dash-dotted line is the true function the continuous line is the approximation with the Bayesian standard deviation plotted by dotted lines (a gradient-like\\napproximation for the output noise based on maximising the likelihood (12) lead us to the\\nvariance with which the data has been generated)\\nFor classification we used the data from the US postal database4 of handwritten zip codes\\ntogether with an RBF kernel The database has 7291 training and 2007 test data of 16 x 16\\ngrey-scale images To apply the classification method to this database 10 binary classification problems were solved and the final output was the class with the largest probability\\nThe same BVs have been considered for each classifier and if a deletion was required the\\nBV having the minimum cumulative score was deleted The cumulative score was chosen to be the maximum of the scores for each classifier Figure 3b shows the test error\\nas a function of the size of the basis set We find that the test error is rather stable over\\na considerable range of basis set sizes Also a comparison with a second sweep through\\nthe data shows that the algorithm seems to have already extracted the relevant information\\nout of the data within a single sweep Using a polynomial kernel for the USPS dataset and\\n500 BVs we achieved a test error of 483% which compares favourably with other sparse\\napproaches [10; 8] but uses smaller basis sets than the SVM (2540 reported in [8])\\nWe also applied our algorithm to the NIST datasetS which contains 60000 data Using a\\nfourth order polynomial kernel with only 500 BVs we achieved a test error of 313% and\\nwe expect that improvements are possible by using a kernel with tunable hyperparameters\\nThe possibility of computing the posterior class probabilities allows us to reject data When\\nthe test data for which the maximum probability was below 05 was rejected the test error\\nwas 153% with 160% of rejection rate\\n\\n4Prom: http://wwwkernel-machinesorg/datahtml\\n5 Available from: http://wwwresearchattcomryann/ocr/rnnist/\\n\\n\\x0c6 Conclusion and further research\\nThis paper presents a sparse approximation for GPs similar to the one found in SVMs [13]\\nor relevance vector machines [10] In contrast to these other approaches our algorithm\\nis fully online and does not construct the sparse representation from the full data set (for\\nsequential optimisation for SVM see [6])\\nAn important open question (besides the issue of model selection) is how to choose the\\nminimal size of the set of basis vectors such that the predictive performance is not much\\ndeteriorated by the approximation involved In fact our numerical classification experiments suggest that the prediction performance is considerably stable when the basis set is\\nabove a certain size It would be interesting if one could relate this minimum size to the\\neffective dimensionality of the problem being defined as the number of feature dimensions\\nwhich are well estimated by the data One may argue as follows: Replacing the true kernel\\nby a modified (finite dimensional) one which contains only the well estimated features will\\nnot change the predictive power On the other hand for kernels with a feature space of\\nfinite dimensionality M it is easy to see that we need never more than M basis vectors\\nbecause of linear dependence Whether such reasoning will lead to a practical procedure\\nfor choosing the appropriate basis set size is a question for further research\\n\\n7 Acknowledgement\\nThis work was supported by EPSRC grant no GRlM81608\\n\\nReferences\\n[1] J M Bernardo and A F Smith Bayesian Theory John Wiley & Sons 1994\\n[2] L Csat6 E Fokoue M Opper B Schottky and O Winther Efficient approaches to Gaussian\\nprocess classification In NIPS volume 12 pages 251- 257 The MIT Press 2000\\n[3] M Gibbs and D J MacKay Efficient implementation of Gaussian processes Technical report\\nhttp://wolraphycamacuklmackay/abstracts/gproshtml 1999\\n[4] J C Lemm J Uhlig and A Weiguny A Bayesian approach to inverse quantum statistics\\nPhysRevLett  84:2006 2000\\n[5] M Opper A Bayesian approach to online learning In Saad [7] pages 363- 378\\n[6] J C Platt Fast training of Support Vector Machines using sequential minimal optimisation In\\nAdvances in Kernel Methods (Support Vector Learning)\\n[7] D Saad editor On-Line Learning in Neural Networks Cambridge Univ Press 1998\\n[8] B Scholkopf S Mika C J Burges P Knirsch K-R Miiller G Ratsch and A J Smola Input\\nspace vs feature space in kernel-based methods IEEE Transactions on Neural Networks\\n10(5):1000-1017 September 1999\\n[9] M Seeger Bayesian model selection for Support Vector Machines Gaussian processes and\\nother kernel classifiers In S A Solla T KLeen and K-R Miiller editors NIPS volume 12 The MIT Press 2000\\n[10] M Tipping The Relevance Vector Machine In S A Solla T KLeen and K-R Miiller\\neditors NIPS volume 12 The MIT Press 2000\\n[11] G F Trecate C K 1 Williams and M Opper Finite-dimensional approximation of Gaussian\\nprocesses In M S Kearns S A Solla and D A Cohn editors NIPS volume 11 The\\nMIT Press 1999\\n[12] v Tresp A Bayesian committee machine Neural Computation accepted\\n[13] V N Vapnik The Nature oj Statistical Learning Theory Springer-Verlag New York NY 1995\\n[14] C K 1 Williams Prediction with Gaussian processes In M 1 Jordan editor Learning in\\nGraphical Models The MIT Press 1999\\n[15] C K 1 Williams and C E Rasmussen Gaussian processes for regression In D S Touretzky\\nM  C Mozer and M  E  Hasselmo editors NIPS volume 8 The MIT Press 1996\\n\\n\\x0c'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4fc3e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_words = list(convert_sentence_to_word(text_to_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26132825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sparse',\n",
       " 'representation',\n",
       " 'for',\n",
       " 'gaussian',\n",
       " 'process',\n",
       " 'models',\n",
       " 'lehel',\n",
       " 'csat',\n",
       " 'and',\n",
       " 'manfred',\n",
       " 'opper',\n",
       " 'neural',\n",
       " 'computing',\n",
       " 'research',\n",
       " 'group',\n",
       " 'school',\n",
       " 'of',\n",
       " 'engineering',\n",
       " 'and',\n",
       " 'applied',\n",
       " 'sciences',\n",
       " 'et',\n",
       " 'birmingham',\n",
       " 'united',\n",
       " 'kingdom',\n",
       " 'csat',\n",
       " 'oppe',\n",
       " 'as',\n",
       " 'ac',\n",
       " 'uk',\n",
       " 'abstract',\n",
       " 'we',\n",
       " 'develop',\n",
       " 'an',\n",
       " 'approach',\n",
       " 'for',\n",
       " 'sparse',\n",
       " 'representation',\n",
       " 'for',\n",
       " 'gaussian',\n",
       " 'process',\n",
       " 'gp',\n",
       " 'models',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'overcome',\n",
       " 'the',\n",
       " 'limitations',\n",
       " 'of',\n",
       " 'gps',\n",
       " 'caused',\n",
       " 'by',\n",
       " 'large',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'the',\n",
       " 'method',\n",
       " 'is',\n",
       " 'based',\n",
       " 'on',\n",
       " 'combination',\n",
       " 'of',\n",
       " 'bayesian',\n",
       " 'online',\n",
       " 'algorithm',\n",
       " 'together',\n",
       " 'with',\n",
       " 'sequential',\n",
       " 'construction',\n",
       " 'of',\n",
       " 'relevant',\n",
       " 'subsample',\n",
       " 'of',\n",
       " 'the',\n",
       " 'data',\n",
       " 'which',\n",
       " 'fully',\n",
       " 'specifies',\n",
       " 'the',\n",
       " 'prediction',\n",
       " 'of',\n",
       " 'the',\n",
       " 'model',\n",
       " 'experimental',\n",
       " 'results',\n",
       " 'on',\n",
       " 'toy',\n",
       " 'examples',\n",
       " 'and',\n",
       " 'large',\n",
       " 'real',\n",
       " 'world',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'indicate',\n",
       " 'the',\n",
       " 'efficiency',\n",
       " 'of',\n",
       " 'the',\n",
       " 'approach',\n",
       " 'introduction',\n",
       " 'gaussian',\n",
       " 'processes',\n",
       " 'gp',\n",
       " 'provide',\n",
       " 'promising',\n",
       " 'non',\n",
       " 'parametric',\n",
       " 'tools',\n",
       " 'for',\n",
       " 'modelling',\n",
       " 'real',\n",
       " 'world',\n",
       " 'statistical',\n",
       " 'problems',\n",
       " 'like',\n",
       " 'other',\n",
       " 'kernel',\n",
       " 'based',\n",
       " 'methods',\n",
       " 'eg',\n",
       " 'support',\n",
       " 'vector',\n",
       " 'machines',\n",
       " 'svms',\n",
       " 'they',\n",
       " 'combine',\n",
       " 'high',\n",
       " 'flexibility',\n",
       " 'ofthe',\n",
       " 'model',\n",
       " 'by',\n",
       " 'working',\n",
       " 'in',\n",
       " 'high',\n",
       " 'often',\n",
       " 'dimensional',\n",
       " 'feature',\n",
       " 'spaces',\n",
       " 'with',\n",
       " 'the',\n",
       " 'simplicity',\n",
       " 'that',\n",
       " 'all',\n",
       " 'operations',\n",
       " 'are',\n",
       " 'kernelized',\n",
       " 'ie',\n",
       " 'they',\n",
       " 'are',\n",
       " 'performed',\n",
       " 'in',\n",
       " 'the',\n",
       " 'lower',\n",
       " 'dimensional',\n",
       " 'input',\n",
       " 'space',\n",
       " 'using',\n",
       " 'positive',\n",
       " 'definite',\n",
       " 'kernels',\n",
       " 'an',\n",
       " 'important',\n",
       " 'advantage',\n",
       " 'of',\n",
       " 'gps',\n",
       " 'over',\n",
       " 'other',\n",
       " 'non',\n",
       " 'bayesian',\n",
       " 'models',\n",
       " 'is',\n",
       " 'the',\n",
       " 'explicit',\n",
       " 'probabilistic',\n",
       " 'formulation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'model',\n",
       " 'this',\n",
       " 'does',\n",
       " 'not',\n",
       " 'only',\n",
       " 'provide',\n",
       " 'the',\n",
       " 'modeller',\n",
       " 'with',\n",
       " 'bayesian',\n",
       " 'confidence',\n",
       " 'intervals',\n",
       " 'for',\n",
       " 'regression',\n",
       " 'or',\n",
       " 'posterior',\n",
       " 'class',\n",
       " 'probabilities',\n",
       " 'for',\n",
       " 'classification',\n",
       " 'but',\n",
       " 'also',\n",
       " 'immediately',\n",
       " 'opens',\n",
       " 'the',\n",
       " 'possibility',\n",
       " 'to',\n",
       " 'treat',\n",
       " 'other',\n",
       " 'nonstandard',\n",
       " 'data',\n",
       " 'models',\n",
       " 'eg',\n",
       " 'quantum',\n",
       " 'inverse',\n",
       " 'statistics',\n",
       " 'unfortunately',\n",
       " 'the',\n",
       " 'drawback',\n",
       " 'of',\n",
       " 'gp',\n",
       " 'models',\n",
       " 'which',\n",
       " 'was',\n",
       " 'originally',\n",
       " 'apparent',\n",
       " 'in',\n",
       " 'svms',\n",
       " 'as',\n",
       " 'well',\n",
       " 'but',\n",
       " 'has',\n",
       " 'now',\n",
       " 'been',\n",
       " 'overcome',\n",
       " 'lies',\n",
       " 'in',\n",
       " 'the',\n",
       " 'huge',\n",
       " 'increase',\n",
       " 'of',\n",
       " 'the',\n",
       " 'computational',\n",
       " 'cost',\n",
       " 'with',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'training',\n",
       " 'data',\n",
       " 'this',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'preclude',\n",
       " 'applications',\n",
       " 'of',\n",
       " 'gps',\n",
       " 'to',\n",
       " 'large',\n",
       " 'datasets',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'presents',\n",
       " 'an',\n",
       " 'approach',\n",
       " 'to',\n",
       " 'overcome',\n",
       " 'this',\n",
       " 'problem',\n",
       " 'it',\n",
       " 'is',\n",
       " 'based',\n",
       " 'on',\n",
       " 'combination',\n",
       " 'of',\n",
       " 'an',\n",
       " 'online',\n",
       " 'learning',\n",
       " 'approach',\n",
       " 'requiring',\n",
       " 'only',\n",
       " 'single',\n",
       " 'sweep',\n",
       " 'through',\n",
       " 'the',\n",
       " 'data',\n",
       " 'and',\n",
       " 'method',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'parameters',\n",
       " 'representing',\n",
       " 'the',\n",
       " 'model',\n",
       " 'making',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'proposed',\n",
       " 'parametrisation',\n",
       " 'the',\n",
       " 'method',\n",
       " 'extracts',\n",
       " 'subset',\n",
       " 'of',\n",
       " 'the',\n",
       " 'examples',\n",
       " 'and',\n",
       " 'the',\n",
       " 'prediction',\n",
       " 'relies',\n",
       " 'only',\n",
       " 'on',\n",
       " 'these',\n",
       " 'basis',\n",
       " 'vectors',\n",
       " 'bv',\n",
       " 'the',\n",
       " 'memory',\n",
       " 'requirement',\n",
       " 'of',\n",
       " 'the',\n",
       " 'algorithm',\n",
       " 'scales',\n",
       " 'thus',\n",
       " 'only',\n",
       " 'with',\n",
       " 'the',\n",
       " 'size',\n",
       " 'of',\n",
       " 'this',\n",
       " 'set',\n",
       " 'experiments',\n",
       " 'with',\n",
       " 'real',\n",
       " 'world',\n",
       " 'datasets',\n",
       " 'confirm',\n",
       " 'the',\n",
       " 'good',\n",
       " 'performance',\n",
       " 'of',\n",
       " 'the',\n",
       " 'proposed',\n",
       " 'method',\n",
       " 'different',\n",
       " 'approach',\n",
       " 'for',\n",
       " 'dealing',\n",
       " 'with',\n",
       " 'large',\n",
       " 'datasets',\n",
       " 'was',\n",
       " 'suggested',\n",
       " 'by',\n",
       " 'tresp',\n",
       " 'his',\n",
       " 'method',\n",
       " 'gaussian',\n",
       " 'process',\n",
       " 'models',\n",
       " 'gps',\n",
       " 'belong',\n",
       " 'to',\n",
       " 'bayesian',\n",
       " 'non',\n",
       " 'parametric',\n",
       " 'models',\n",
       " 'where',\n",
       " 'likelihoods',\n",
       " 'are',\n",
       " 'parametrised',\n",
       " 'by',\n",
       " 'gaussian',\n",
       " 'stochastic',\n",
       " 'process',\n",
       " 'random',\n",
       " 'field',\n",
       " 'which',\n",
       " 'is',\n",
       " 'indexed',\n",
       " 'by',\n",
       " 'the',\n",
       " 'continuous',\n",
       " 'input',\n",
       " 'variable',\n",
       " 'the',\n",
       " 'prior',\n",
       " 'knowledge',\n",
       " 'about',\n",
       " 'is',\n",
       " 'expressed',\n",
       " 'in',\n",
       " 'the',\n",
       " 'prior',\n",
       " 'mean',\n",
       " 'and',\n",
       " 'the',\n",
       " 'covariance',\n",
       " 'given',\n",
       " 'by',\n",
       " 'the',\n",
       " 'kernel',\n",
       " 'ko',\n",
       " 'xx',\n",
       " 'cov',\n",
       " 'in',\n",
       " 'the',\n",
       " 'following',\n",
       " 'only',\n",
       " 'zero',\n",
       " 'mean',\n",
       " 'gp',\n",
       " 'priors',\n",
       " 'are',\n",
       " 'used',\n",
       " 'in',\n",
       " 'supervised',\n",
       " 'learning',\n",
       " 'the',\n",
       " 'process',\n",
       " 'is',\n",
       " 'used',\n",
       " 'as',\n",
       " 'latent',\n",
       " 'variable',\n",
       " 'in',\n",
       " 'the',\n",
       " 'likelihood',\n",
       " 'yla',\n",
       " 'which',\n",
       " 'denotes',\n",
       " 'the',\n",
       " 'probability',\n",
       " 'of',\n",
       " 'output',\n",
       " 'given',\n",
       " 'the',\n",
       " 'input',\n",
       " 'based',\n",
       " 'on',\n",
       " 'set',\n",
       " 'of',\n",
       " 'input',\n",
       " 'output',\n",
       " 'pairs',\n",
       " 'xn',\n",
       " 'yn',\n",
       " 'with',\n",
       " 'xn',\n",
       " 'and',\n",
       " 'yn',\n",
       " 'the',\n",
       " 'bayesian',\n",
       " 'learning',\n",
       " 'method',\n",
       " 'computes',\n",
       " 'the',\n",
       " 'posterior',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'the',\n",
       " 'process',\n",
       " 'using',\n",
       " 'the',\n",
       " 'prior',\n",
       " 'and',\n",
       " 'likelihood',\n",
       " 'although',\n",
       " 'the',\n",
       " 'prior',\n",
       " 'is',\n",
       " 'gaussian',\n",
       " 'process',\n",
       " 'the',\n",
       " 'posterior',\n",
       " 'process',\n",
       " 'usually',\n",
       " 'is',\n",
       " 'not',\n",
       " 'gaussian',\n",
       " 'except',\n",
       " 'for',\n",
       " 'the',\n",
       " 'special',\n",
       " 'case',\n",
       " 'of',\n",
       " 'regression',\n",
       " 'with',\n",
       " 'gaussian',\n",
       " 'noise',\n",
       " 'nevertheless',\n",
       " 'various',\n",
       " 'approaches',\n",
       " 'have',\n",
       " 'been',\n",
       " 'introduced',\n",
       " 'recently',\n",
       " 'to',\n",
       " 'approximate',\n",
       " 'the',\n",
       " 'posterior',\n",
       " 'averages',\n",
       " 'our',\n",
       " 'approach',\n",
       " 'is',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'idea',\n",
       " 'of',\n",
       " 'approximating',\n",
       " 'the',\n",
       " 'true',\n",
       " 'posterior',\n",
       " 'process',\n",
       " 'by',\n",
       " 'gaussian',\n",
       " 'process',\n",
       " 'which',\n",
       " 'is',\n",
       " 'fully',\n",
       " 'specified',\n",
       " 'by',\n",
       " 'covariance',\n",
       " 'kernel',\n",
       " 'kt',\n",
       " 'xx',\n",
       " 'and',\n",
       " 'posterior',\n",
       " 'mean',\n",
       " 'where',\n",
       " 'is',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'training',\n",
       " 'data',\n",
       " 'processed',\n",
       " 'by',\n",
       " 'the',\n",
       " 'algorithm',\n",
       " 'so',\n",
       " 'far',\n",
       " 'such',\n",
       " 'an',\n",
       " 'approximation',\n",
       " 'could',\n",
       " 'be',\n",
       " 'formulated',\n",
       " 'within',\n",
       " 'the',\n",
       " 'variational',\n",
       " 'approach',\n",
       " 'where',\n",
       " 'is',\n",
       " 'chosen',\n",
       " 'such',\n",
       " 'that',\n",
       " 'the',\n",
       " 'relative',\n",
       " 'entropy',\n",
       " 'qp',\n",
       " 'eq',\n",
       " 'in',\n",
       " 'is',\n",
       " 'minimal',\n",
       " 'however',\n",
       " 'in',\n",
       " 'this',\n",
       " 'formulation',\n",
       " 'the',\n",
       " 'expectation',\n",
       " 'is',\n",
       " 'over',\n",
       " 'the',\n",
       " 'approximate',\n",
       " 'process',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'over',\n",
       " 'it',\n",
       " 'seems',\n",
       " 'intuitively',\n",
       " 'better',\n",
       " 'to',\n",
       " 'minimise',\n",
       " 'the',\n",
       " 'other',\n",
       " 'kl',\n",
       " 'divergence',\n",
       " 'given',\n",
       " 'by',\n",
       " 'ep',\n",
       " 'in',\n",
       " 'because',\n",
       " 'the',\n",
       " 'expectation',\n",
       " 'is',\n",
       " 'over',\n",
       " 'the',\n",
       " 'true',\n",
       " 'distribution',\n",
       " 'unfortunately',\n",
       " 'such',\n",
       " 'computation',\n",
       " 'is',\n",
       " 'generally',\n",
       " 'not',\n",
       " 'possible',\n",
       " 'the',\n",
       " 'following',\n",
       " 'online',\n",
       " 'approach',\n",
       " 'can',\n",
       " 'be',\n",
       " 'understood',\n",
       " 'as',\n",
       " 'an',\n",
       " 'approximation',\n",
       " 'to',\n",
       " 'this',\n",
       " 'task',\n",
       " 'online',\n",
       " 'learning',\n",
       " 'for',\n",
       " 'gaussian',\n",
       " 'processes',\n",
       " 'in',\n",
       " 'this',\n",
       " 'section',\n",
       " 'we',\n",
       " 'briefly',\n",
       " 'review',\n",
       " 'the',\n",
       " 'main',\n",
       " 'idea',\n",
       " 'of',\n",
       " 'the',\n",
       " 'bayesian',\n",
       " 'online',\n",
       " 'approach',\n",
       " 'see',\n",
       " 'eg',\n",
       " 'to',\n",
       " 'gp',\n",
       " 'models',\n",
       " 'we',\n",
       " 'process',\n",
       " 'the',\n",
       " 'training',\n",
       " 'data',\n",
       " 'sequentially',\n",
       " 'one',\n",
       " 'after',\n",
       " 'the',\n",
       " 'other',\n",
       " 'assume',\n",
       " 'we',\n",
       " 'have',\n",
       " 'gaussian',\n",
       " 'approximation',\n",
       " 'to',\n",
       " 'the',\n",
       " 'posterior',\n",
       " 'process',\n",
       " 'at',\n",
       " 'time',\n",
       " 'we',\n",
       " 'use',\n",
       " 'the',\n",
       " 'next',\n",
       " 'example',\n",
       " 'to',\n",
       " 'update',\n",
       " 'the',\n",
       " 'posterior',\n",
       " 'using',\n",
       " 'bayes',\n",
       " 'rule',\n",
       " 'via',\n",
       " 'yt',\n",
       " 'la',\n",
       " 'xt',\n",
       " 'pt',\n",
       " 'yt',\n",
       " 'la',\n",
       " 'xt',\n",
       " 'since',\n",
       " 'the',\n",
       " 'resulting',\n",
       " 'posterior',\n",
       " 'is',\n",
       " 'non',\n",
       " 'gaussian',\n",
       " 'we',\n",
       " 'project',\n",
       " 'it',\n",
       " 'to',\n",
       " 'the',\n",
       " 'closest',\n",
       " 'gaussian',\n",
       " 'process',\n",
       " 'which',\n",
       " 'minimises',\n",
       " 'the',\n",
       " 'kl',\n",
       " 'divergence',\n",
       " 'note',\n",
       " 'that',\n",
       " 'now',\n",
       " 'the',\n",
       " 'new',\n",
       " 'approximation',\n",
       " 'is',\n",
       " 'on',\n",
       " 'correct',\n",
       " 'side',\n",
       " 'of',\n",
       " 'the',\n",
       " 'kl',\n",
       " 'divergence',\n",
       " 'the',\n",
       " 'minimisation',\n",
       " 'can',\n",
       " 'be',\n",
       " 'performed',\n",
       " 'exactly',\n",
       " 'leading',\n",
       " 'to',\n",
       " 'match',\n",
       " 'of',\n",
       " 'the',\n",
       " 'means',\n",
       " 'and',\n",
       " 'covariances',\n",
       " 'of',\n",
       " 'and',\n",
       " 'since',\n",
       " 'is',\n",
       " 'much',\n",
       " 'less',\n",
       " 'complex',\n",
       " 'than',\n",
       " 'the',\n",
       " 'full',\n",
       " 'posterior',\n",
       " 'it',\n",
       " 'is',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'write',\n",
       " 'down',\n",
       " 'the',\n",
       " 'changes',\n",
       " 'in',\n",
       " 'the',\n",
       " 'first',\n",
       " 'two',\n",
       " 'moments',\n",
       " 'analytically',\n",
       " 'kt',\n",
       " 'xxt',\n",
       " 'xx',\n",
       " 'kt',\n",
       " 'xx',\n",
       " 'xxt',\n",
       " 'kt',\n",
       " 'xt',\n",
       " 'where',\n",
       " 'the',\n",
       " 'scalar',\n",
       " 'coefficients',\n",
       " 'and',\n",
       " 'are',\n",
       " 'with',\n",
       " 'averaging',\n",
       " 'performed',\n",
       " 'with',\n",
       " 'respect',\n",
       " 'to',\n",
       " 'the',\n",
       " 'marginal',\n",
       " 'gaussian',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'the',\n",
       " 'process',\n",
       " 'variable',\n",
       " 'at',\n",
       " 'input',\n",
       " 'xt',\n",
       " 'note',\n",
       " 'that',\n",
       " 'this',\n",
       " 'yields',\n",
       " 'one',\n",
       " 'dimensional',\n",
       " 'integral',\n",
       " 'derivatives',\n",
       " 'are',\n",
       " 'is',\n",
       " 'based',\n",
       " 'on',\n",
       " 'splitting',\n",
       " 'the',\n",
       " 'data',\n",
       " 'set',\n",
       " 'into',\n",
       " 'smaller',\n",
       " 'subsets',\n",
       " 'and',\n",
       " 'training',\n",
       " 'individual',\n",
       " 'gp',\n",
       " 'predictors',\n",
       " 'on',\n",
       " 'each',\n",
       " 'of',\n",
       " 'them',\n",
       " 'the',\n",
       " 'final',\n",
       " 'prediction',\n",
       " 'is',\n",
       " 'achieved',\n",
       " 'by',\n",
       " 'specific',\n",
       " 'weighting',\n",
       " 'of',\n",
       " 'the',\n",
       " 'individual',\n",
       " 'predictors',\n",
       " 'ph',\n",
       " 'es',\n",
       " 'figure',\n",
       " 'projection',\n",
       " 'of',\n",
       " 'the',\n",
       " 'new',\n",
       " 'input',\n",
       " 'pt',\n",
       " 'to',\n",
       " 'the',\n",
       " 'subspace',\n",
       " 'spanned',\n",
       " 'by',\n",
       " 'previous',\n",
       " 'inputs',\n",
       " 'is',\n",
       " 'the',\n",
       " 'projection',\n",
       " 'to',\n",
       " 'the',\n",
       " 'linear',\n",
       " 'span',\n",
       " 'of',\n",
       " 'pih',\n",
       " 'and',\n",
       " 'pres',\n",
       " 'the',\n",
       " 'residual',\n",
       " 'vector',\n",
       " 'subfigure',\n",
       " 'shows',\n",
       " 'the',\n",
       " 'projections',\n",
       " 'to',\n",
       " 'the',\n",
       " 'subspace',\n",
       " 'and',\n",
       " 'gives',\n",
       " 'geometric',\n",
       " 'picture',\n",
       " 'of',\n",
       " 'the',\n",
       " 'measurable',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'error',\n",
       " 'it',\n",
       " 'from',\n",
       " 'eq',\n",
       " 'taken',\n",
       " 'with',\n",
       " 'respect',\n",
       " 'to',\n",
       " 'note',\n",
       " 'also',\n",
       " 'that',\n",
       " 'this',\n",
       " 'procedure',\n",
       " 'does',\n",
       " 'not',\n",
       " 'equal',\n",
       " 'the',\n",
       " 'extended',\n",
       " 'kalman',\n",
       " 'filter',\n",
       " 'which',\n",
       " 'involves',\n",
       " 'linearisations',\n",
       " 'of',\n",
       " 'likelihoods',\n",
       " 'whereas',\n",
       " 'in',\n",
       " 'our',\n",
       " 'approach',\n",
       " 'it',\n",
       " 'is',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'use',\n",
       " 'non',\n",
       " 'smooth',\n",
       " 'likelihoods',\n",
       " 'eg',\n",
       " 'noise',\n",
       " 'free',\n",
       " 'classifications',\n",
       " 'without',\n",
       " 'problems',\n",
       " 'it',\n",
       " 'turns',\n",
       " 'out',\n",
       " 'that',\n",
       " 'the',\n",
       " 'recursion',\n",
       " 'is',\n",
       " 'solved',\n",
       " 'by',\n",
       " 'the',\n",
       " 'parametrisation',\n",
       " 'kt',\n",
       " 'xx',\n",
       " 'iko',\n",
       " 'xxi',\n",
       " 'at',\n",
       " 'ko',\n",
       " 'xx',\n",
       " 'll',\n",
       " 'iko',\n",
       " 'xxi',\n",
       " 'ct',\n",
       " 'ij',\n",
       " 'ko',\n",
       " 'xjx',\n",
       " 'such',\n",
       " 'that',\n",
       " 'in',\n",
       " 'each',\n",
       " 'on',\n",
       " 'line',\n",
       " 'step',\n",
       " 'we',\n",
       " 'have',\n",
       " 'to',\n",
       " 'update',\n",
       " 'only',\n",
       " 'the',\n",
       " 'vector',\n",
       " 'of',\n",
       " 'and',\n",
       " 'the',\n",
       " 'matrix',\n",
       " 'of',\n",
       " 'for',\n",
       " 'notational',\n",
       " 'convenience',\n",
       " 'we',\n",
       " 'use',\n",
       " 'vector',\n",
       " 'at',\n",
       " 'at',\n",
       " 'at',\n",
       " 'jt',\n",
       " 'and',\n",
       " 'matrix',\n",
       " 'ct',\n",
       " 'ij',\n",
       " 'hj',\n",
       " 'in',\n",
       " 'zero',\n",
       " 'mean',\n",
       " 'gp',\n",
       " 'with',\n",
       " 'kernel',\n",
       " 'ko',\n",
       " 'is',\n",
       " 'used',\n",
       " 'as',\n",
       " 'the',\n",
       " 'starting',\n",
       " 'point',\n",
       " 'for',\n",
       " 'the',\n",
       " 'algorithm',\n",
       " 'ao',\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40e90eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "clean_word = remove_stopwords(text_as_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5ef6949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sparse',\n",
       " 'representation',\n",
       " 'gaussian',\n",
       " 'process',\n",
       " 'models',\n",
       " 'lehel',\n",
       " 'csat',\n",
       " 'manfred',\n",
       " 'opper',\n",
       " 'neural',\n",
       " 'computing',\n",
       " 'research',\n",
       " 'group',\n",
       " 'school',\n",
       " 'engineering',\n",
       " 'applied',\n",
       " 'sciences',\n",
       " 'et',\n",
       " 'birmingham',\n",
       " 'united',\n",
       " 'kingdom',\n",
       " 'csat',\n",
       " 'oppe',\n",
       " 'ac',\n",
       " 'uk',\n",
       " 'abstract',\n",
       " 'develop',\n",
       " 'approach',\n",
       " 'sparse',\n",
       " 'representation',\n",
       " 'gaussian',\n",
       " 'process',\n",
       " 'gp',\n",
       " 'models',\n",
       " 'order',\n",
       " 'overcome',\n",
       " 'limitations',\n",
       " 'gps',\n",
       " 'caused',\n",
       " 'large',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'method',\n",
       " 'based',\n",
       " 'combination',\n",
       " 'bayesian',\n",
       " 'online',\n",
       " 'algorithm',\n",
       " 'together',\n",
       " 'sequential',\n",
       " 'construction',\n",
       " 'relevant',\n",
       " 'subsample',\n",
       " 'data',\n",
       " 'fully',\n",
       " 'specifies',\n",
       " 'prediction',\n",
       " 'model',\n",
       " 'experimental',\n",
       " 'results',\n",
       " 'toy',\n",
       " 'examples',\n",
       " 'large',\n",
       " 'real',\n",
       " 'world',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'indicate',\n",
       " 'efficiency',\n",
       " 'approach',\n",
       " 'introduction',\n",
       " 'gaussian',\n",
       " 'processes',\n",
       " 'gp',\n",
       " 'provide',\n",
       " 'promising',\n",
       " 'non',\n",
       " 'parametric',\n",
       " 'tools',\n",
       " 'modelling',\n",
       " 'real',\n",
       " 'world',\n",
       " 'statistical',\n",
       " 'problems',\n",
       " 'like',\n",
       " 'kernel',\n",
       " 'based',\n",
       " 'methods',\n",
       " 'eg',\n",
       " 'support',\n",
       " 'vector',\n",
       " 'machines',\n",
       " 'svms',\n",
       " 'combine',\n",
       " 'high',\n",
       " 'flexibility',\n",
       " 'ofthe',\n",
       " 'model',\n",
       " 'working',\n",
       " 'high',\n",
       " 'often',\n",
       " 'dimensional',\n",
       " 'feature',\n",
       " 'spaces',\n",
       " 'simplicity',\n",
       " 'operations',\n",
       " 'kernelized',\n",
       " 'ie',\n",
       " 'performed',\n",
       " 'lower',\n",
       " 'dimensional',\n",
       " 'input',\n",
       " 'space',\n",
       " 'using',\n",
       " 'positive',\n",
       " 'definite',\n",
       " 'kernels',\n",
       " 'important',\n",
       " 'advantage',\n",
       " 'gps',\n",
       " 'non',\n",
       " 'bayesian',\n",
       " 'models',\n",
       " 'explicit',\n",
       " 'probabilistic',\n",
       " 'formulation',\n",
       " 'model',\n",
       " 'provide',\n",
       " 'modeller',\n",
       " 'bayesian',\n",
       " 'confidence',\n",
       " 'intervals',\n",
       " 'regression',\n",
       " 'posterior',\n",
       " 'class',\n",
       " 'probabilities',\n",
       " 'classification',\n",
       " 'also',\n",
       " 'immediately',\n",
       " 'opens',\n",
       " 'possibility',\n",
       " 'treat',\n",
       " 'nonstandard',\n",
       " 'data',\n",
       " 'models',\n",
       " 'eg',\n",
       " 'quantum',\n",
       " 'inverse',\n",
       " 'statistics',\n",
       " 'unfortunately',\n",
       " 'drawback',\n",
       " 'gp',\n",
       " 'models',\n",
       " 'originally',\n",
       " 'apparent',\n",
       " 'svms',\n",
       " 'well',\n",
       " 'overcome',\n",
       " 'lies',\n",
       " 'huge',\n",
       " 'increase',\n",
       " 'computational',\n",
       " 'cost',\n",
       " 'number',\n",
       " 'training',\n",
       " 'data',\n",
       " 'seems',\n",
       " 'preclude',\n",
       " 'applications',\n",
       " 'gps',\n",
       " 'large',\n",
       " 'datasets',\n",
       " 'paper',\n",
       " 'presents',\n",
       " 'approach',\n",
       " 'overcome',\n",
       " 'problem',\n",
       " 'based',\n",
       " 'combination',\n",
       " 'online',\n",
       " 'learning',\n",
       " 'approach',\n",
       " 'requiring',\n",
       " 'single',\n",
       " 'sweep',\n",
       " 'data',\n",
       " 'method',\n",
       " 'reduce',\n",
       " 'number',\n",
       " 'parameters',\n",
       " 'representing',\n",
       " 'model',\n",
       " 'making',\n",
       " 'use',\n",
       " 'proposed',\n",
       " 'parametrisation',\n",
       " 'method',\n",
       " 'extracts',\n",
       " 'subset',\n",
       " 'examples',\n",
       " 'prediction',\n",
       " 'relies',\n",
       " 'basis',\n",
       " 'vectors',\n",
       " 'bv',\n",
       " 'memory',\n",
       " 'requirement',\n",
       " 'algorithm',\n",
       " 'scales',\n",
       " 'thus',\n",
       " 'size',\n",
       " 'set',\n",
       " 'experiments',\n",
       " 'real',\n",
       " 'world',\n",
       " 'datasets',\n",
       " 'confirm',\n",
       " 'good',\n",
       " 'performance',\n",
       " 'proposed',\n",
       " 'method',\n",
       " 'different',\n",
       " 'approach',\n",
       " 'dealing',\n",
       " 'large',\n",
       " 'datasets',\n",
       " 'suggested',\n",
       " 'tresp',\n",
       " 'method',\n",
       " 'gaussian',\n",
       " 'process',\n",
       " 'models',\n",
       " 'gps',\n",
       " 'belong',\n",
       " 'bayesian',\n",
       " 'non',\n",
       " 'parametric',\n",
       " 'models',\n",
       " 'likelihoods',\n",
       " 'parametrised',\n",
       " 'gaussian',\n",
       " 'stochastic',\n",
       " 'process',\n",
       " 'random',\n",
       " 'field',\n",
       " 'indexed',\n",
       " 'continuous',\n",
       " 'input',\n",
       " 'variable',\n",
       " 'prior',\n",
       " 'knowledge',\n",
       " 'expressed',\n",
       " 'prior',\n",
       " 'mean',\n",
       " 'covariance',\n",
       " 'given',\n",
       " 'kernel',\n",
       " 'ko',\n",
       " 'xx',\n",
       " 'cov',\n",
       " 'following',\n",
       " 'zero',\n",
       " 'mean',\n",
       " 'gp',\n",
       " 'priors',\n",
       " 'used',\n",
       " 'supervised',\n",
       " 'learning',\n",
       " 'process',\n",
       " 'used',\n",
       " 'latent',\n",
       " 'variable',\n",
       " 'likelihood',\n",
       " 'yla',\n",
       " 'denotes',\n",
       " 'probability',\n",
       " 'output',\n",
       " 'given',\n",
       " 'input',\n",
       " 'based',\n",
       " 'set',\n",
       " 'input',\n",
       " 'output',\n",
       " 'pairs',\n",
       " 'xn',\n",
       " 'yn',\n",
       " 'xn',\n",
       " 'yn',\n",
       " 'bayesian',\n",
       " 'learning',\n",
       " 'method',\n",
       " 'computes',\n",
       " 'posterior',\n",
       " 'distribution',\n",
       " 'process',\n",
       " 'using',\n",
       " 'prior',\n",
       " 'likelihood',\n",
       " 'although',\n",
       " 'prior',\n",
       " 'gaussian',\n",
       " 'process',\n",
       " 'posterior',\n",
       " 'process',\n",
       " 'usually',\n",
       " 'gaussian',\n",
       " 'except',\n",
       " 'special',\n",
       " 'case',\n",
       " 'regression',\n",
       " 'gaussian',\n",
       " 'noise',\n",
       " 'nevertheless',\n",
       " 'various',\n",
       " 'approaches',\n",
       " 'introduced',\n",
       " 'recently',\n",
       " 'approximate',\n",
       " 'posterior',\n",
       " 'averages',\n",
       " 'approach',\n",
       " 'based',\n",
       " 'idea',\n",
       " 'approximating',\n",
       " 'true',\n",
       " 'posterior',\n",
       " 'process',\n",
       " 'gaussian',\n",
       " 'process',\n",
       " 'fully',\n",
       " 'specified',\n",
       " 'covariance',\n",
       " 'kernel',\n",
       " 'kt',\n",
       " 'xx',\n",
       " 'posterior',\n",
       " 'mean',\n",
       " 'number',\n",
       " 'training',\n",
       " 'data',\n",
       " 'processed',\n",
       " 'algorithm',\n",
       " 'far',\n",
       " 'approximation',\n",
       " 'could',\n",
       " 'formulated',\n",
       " 'within',\n",
       " 'variational',\n",
       " 'approach',\n",
       " 'chosen',\n",
       " 'relative',\n",
       " 'entropy',\n",
       " 'qp',\n",
       " 'eq',\n",
       " 'minimal',\n",
       " 'however',\n",
       " 'formulation',\n",
       " 'expectation',\n",
       " 'approximate',\n",
       " 'process',\n",
       " 'rather',\n",
       " 'seems',\n",
       " 'intuitively',\n",
       " 'better',\n",
       " 'minimise',\n",
       " 'kl',\n",
       " 'divergence',\n",
       " 'given',\n",
       " 'ep',\n",
       " 'expectation',\n",
       " 'true',\n",
       " 'distribution',\n",
       " 'unfortunately',\n",
       " 'computation',\n",
       " 'generally',\n",
       " 'possible',\n",
       " 'following',\n",
       " 'online',\n",
       " 'approach',\n",
       " 'understood',\n",
       " 'approximation',\n",
       " 'task',\n",
       " 'online',\n",
       " 'learning',\n",
       " 'gaussian',\n",
       " 'processes',\n",
       " 'section',\n",
       " 'briefly',\n",
       " 'review',\n",
       " 'main',\n",
       " 'idea',\n",
       " 'bayesian',\n",
       " 'online',\n",
       " 'approach',\n",
       " 'see',\n",
       " 'eg',\n",
       " 'gp',\n",
       " 'models',\n",
       " 'process',\n",
       " 'training',\n",
       " 'data',\n",
       " 'sequentially',\n",
       " 'one',\n",
       " 'assume',\n",
       " 'gaussian',\n",
       " 'approximation',\n",
       " 'posterior',\n",
       " 'process',\n",
       " 'time',\n",
       " 'use',\n",
       " 'next',\n",
       " 'example',\n",
       " 'update',\n",
       " 'posterior',\n",
       " 'using',\n",
       " 'bayes',\n",
       " 'rule',\n",
       " 'via',\n",
       " 'yt',\n",
       " 'la',\n",
       " 'xt',\n",
       " 'pt',\n",
       " 'yt',\n",
       " 'la',\n",
       " 'xt',\n",
       " 'since',\n",
       " 'resulting',\n",
       " 'posterior',\n",
       " 'non',\n",
       " 'gaussian',\n",
       " 'project',\n",
       " 'closest',\n",
       " 'gaussian',\n",
       " 'process',\n",
       " 'minimises',\n",
       " 'kl',\n",
       " 'divergence',\n",
       " 'note',\n",
       " 'new',\n",
       " 'approximation',\n",
       " 'correct',\n",
       " 'side',\n",
       " 'kl',\n",
       " 'divergence',\n",
       " 'minimisation',\n",
       " 'performed',\n",
       " 'exactly',\n",
       " 'leading',\n",
       " 'match',\n",
       " 'means',\n",
       " 'covariances',\n",
       " 'since',\n",
       " 'much',\n",
       " 'less',\n",
       " 'complex',\n",
       " 'full',\n",
       " 'posterior',\n",
       " 'possible',\n",
       " 'write',\n",
       " 'changes',\n",
       " 'first',\n",
       " 'two',\n",
       " 'moments',\n",
       " 'analytically',\n",
       " 'kt',\n",
       " 'xxt',\n",
       " 'xx',\n",
       " 'kt',\n",
       " 'xx',\n",
       " 'xxt',\n",
       " 'kt',\n",
       " 'xt',\n",
       " 'scalar',\n",
       " 'coefficients',\n",
       " 'averaging',\n",
       " 'performed',\n",
       " 'respect',\n",
       " 'marginal',\n",
       " 'gaussian',\n",
       " 'distribution',\n",
       " 'process',\n",
       " 'variable',\n",
       " 'input',\n",
       " 'xt',\n",
       " 'note',\n",
       " 'yields',\n",
       " 'one',\n",
       " 'dimensional',\n",
       " 'integral',\n",
       " 'derivatives',\n",
       " 'based',\n",
       " 'splitting',\n",
       " 'data',\n",
       " 'set',\n",
       " 'smaller',\n",
       " 'subsets',\n",
       " 'training',\n",
       " 'individual',\n",
       " 'gp',\n",
       " 'predictors',\n",
       " 'final',\n",
       " 'prediction',\n",
       " 'achieved',\n",
       " 'specific',\n",
       " 'weighting',\n",
       " 'individual',\n",
       " 'predictors',\n",
       " 'ph',\n",
       " 'es',\n",
       " 'figure',\n",
       " 'projection',\n",
       " 'new',\n",
       " 'input',\n",
       " 'pt',\n",
       " 'subspace',\n",
       " 'spanned',\n",
       " 'previous',\n",
       " 'inputs',\n",
       " 'projection',\n",
       " 'linear',\n",
       " 'span',\n",
       " 'pih',\n",
       " 'pres',\n",
       " 'residual',\n",
       " 'vector',\n",
       " 'subfigure',\n",
       " 'shows',\n",
       " 'projections',\n",
       " 'subspace',\n",
       " 'gives',\n",
       " 'geometric',\n",
       " 'picture',\n",
       " 'measurable',\n",
       " 'part',\n",
       " 'error',\n",
       " 'eq',\n",
       " 'taken',\n",
       " 'respect',\n",
       " 'note',\n",
       " 'also',\n",
       " 'procedure',\n",
       " 'equal',\n",
       " 'extended',\n",
       " 'kalman',\n",
       " 'filter',\n",
       " 'involves',\n",
       " 'linearisations',\n",
       " 'likelihoods',\n",
       " 'whereas',\n",
       " 'approach',\n",
       " 'possible',\n",
       " 'use',\n",
       " 'non',\n",
       " 'smooth',\n",
       " 'likelihoods',\n",
       " 'eg',\n",
       " 'noise',\n",
       " 'free',\n",
       " 'classifications',\n",
       " 'without',\n",
       " 'problems',\n",
       " 'turns',\n",
       " 'recursion',\n",
       " 'solved',\n",
       " 'parametrisation',\n",
       " 'kt',\n",
       " 'xx',\n",
       " 'iko',\n",
       " 'xxi',\n",
       " 'ko',\n",
       " 'xx',\n",
       " 'iko',\n",
       " 'xxi',\n",
       " 'ct',\n",
       " 'ij',\n",
       " 'ko',\n",
       " 'xjx',\n",
       " 'line',\n",
       " 'step',\n",
       " 'update',\n",
       " 'vector',\n",
       " 'matrix',\n",
       " 'notational',\n",
       " 'convenience',\n",
       " 'use',\n",
       " 'vector',\n",
       " 'jt',\n",
       " 'matrix',\n",
       " 'ct',\n",
       " 'ij',\n",
       " 'hj',\n",
       " 'zero',\n",
       " 'mean',\n",
       " 'gp',\n",
       " 'kernel',\n",
       " 'ko',\n",
       " 'used',\n",
       " 'starting',\n",
       " 'point',\n",
       " 'algorithm',\n",
       " 'ao',\n",
       " 'co',\n",
       " 'starting',\n",
       " 'parameters',\n",
       " 'update',\n",
       " 'parameters',\n",
       " 'defined',\n",
       " 'found',\n",
       " 'bl',\n",
       " 'ctkt',\n",
       " 'et',\n",
       " 'tkt',\n",
       " 'et',\n",
       " 'tkt',\n",
       " 'et',\n",
       " 'lf',\n",
       " 'kt',\n",
       " 'ko',\n",
       " 'xixt',\n",
       " 'ko',\n",
       " 'xt',\n",
       " 'xt',\n",
       " 'jt',\n",
       " 'et',\n",
       " 'th',\n",
       " 'unit',\n",
       " 'vector',\n",
       " 'components',\n",
       " 'except',\n",
       " 'th',\n",
       " 'zero',\n",
       " 'scalar',\n",
       " 'coefficients',\n",
       " 'bl',\n",
       " 'computed',\n",
       " 'serious',\n",
       " 'drawback',\n",
       " 'approach',\n",
       " 'shares',\n",
       " 'many',\n",
       " 'kernel',\n",
       " 'methods',\n",
       " 'quadratic',\n",
       " 'increase',\n",
       " 'matrix',\n",
       " 'size',\n",
       " 'training',\n",
       " 'data',\n",
       " 'sparse',\n",
       " 'representation',\n",
       " 'use',\n",
       " 'following',\n",
       " 'idea',\n",
       " 'reducing',\n",
       " 'increase',\n",
       " 'size',\n",
       " 'similar',\n",
       " 'approach',\n",
       " 'see',\n",
       " 'consider',\n",
       " 'feature',\n",
       " 'expansion',\n",
       " 'kernel',\n",
       " 'ko',\n",
       " 'xx',\n",
       " 'decompose',\n",
       " 'new',\n",
       " 'feature',\n",
       " 'vector',\n",
       " 'xt',\n",
       " 'linear',\n",
       " 'combination',\n",
       " 'previous',\n",
       " 'features',\n",
       " 'residual',\n",
       " 'pres',\n",
       " 'xt',\n",
       " 'pt',\n",
       " 'pres',\n",
       " 'ei',\n",
       " 'xi',\n",
       " 'pres',\n",
       " 'projection',\n",
       " 'pt',\n",
       " 'previous',\n",
       " 'inputs',\n",
       " 'et',\n",
       " 'el',\n",
       " 'etjt',\n",
       " 'coordinates',\n",
       " 'respect',\n",
       " 'basis',\n",
       " 'pih',\n",
       " 'lt',\n",
       " 'express',\n",
       " 'gp',\n",
       " 'means',\n",
       " 'qt',\n",
       " 'et',\n",
       " 'yhi',\n",
       " 'residual',\n",
       " 'novelty',\n",
       " 'factor',\n",
       " 'associated',\n",
       " 'new',\n",
       " 'feature',\n",
       " 'vector',\n",
       " 'vector',\n",
       " 'et',\n",
       " 'residual',\n",
       " 'term',\n",
       " 'yt',\n",
       " 'expressed',\n",
       " 'terms',\n",
       " 'kernels',\n",
       " 'et',\n",
       " 'hi',\n",
       " 'yt',\n",
       " 'kt',\n",
       " 'kb',\n",
       " 'ij',\n",
       " 'ko',\n",
       " 'xixj',\n",
       " 'hj',\n",
       " 'lt',\n",
       " 'kt',\n",
       " 'xt',\n",
       " 'xt',\n",
       " 'relation',\n",
       " 'quantities',\n",
       " 'et',\n",
       " 'yt',\n",
       " 'illustrated',\n",
       " 'figure',\n",
       " 'neglecting',\n",
       " 'last',\n",
       " 'term',\n",
       " 'decomposition',\n",
       " 'new',\n",
       " 'input',\n",
       " 'performing',\n",
       " 'update',\n",
       " 'resulting',\n",
       " 'vector',\n",
       " 'equivalent',\n",
       " 'update',\n",
       " 'rule',\n",
       " 'et',\n",
       " 'replaced',\n",
       " 'et',\n",
       " 'note',\n",
       " 'dimension',\n",
       " 'parameter',\n",
       " 'space',\n",
       " 'increased',\n",
       " 'approximative',\n",
       " 'update',\n",
       " 'memory',\n",
       " 'required',\n",
       " 'algorithm',\n",
       " 'scales',\n",
       " 'quadratically',\n",
       " 'size',\n",
       " 'set',\n",
       " 'basis',\n",
       " 'vectors',\n",
       " 'ie',\n",
       " 'examples',\n",
       " 'full',\n",
       " 'update',\n",
       " 'made',\n",
       " 'similar',\n",
       " 'support',\n",
       " 'vectors',\n",
       " 'without',\n",
       " 'need',\n",
       " 'solve',\n",
       " 'high',\n",
       " 'dimensional',\n",
       " 'convex',\n",
       " 'optimisation',\n",
       " 'problem',\n",
       " 'also',\n",
       " 'related',\n",
       " 'kernel',\n",
       " 'pca',\n",
       " 'reduced',\n",
       " 'set',\n",
       " 'method',\n",
       " 'full',\n",
       " 'solution',\n",
       " 'computed',\n",
       " 'first',\n",
       " 'reduced',\n",
       " 'set',\n",
       " 'used',\n",
       " 'prediction',\n",
       " 'replacing',\n",
       " 'input',\n",
       " 'vector',\n",
       " 'cjl',\n",
       " 'projection',\n",
       " 'linear',\n",
       " 'span',\n",
       " 'bvs',\n",
       " 'updating',\n",
       " 'gp',\n",
       " 'parameters',\n",
       " 'induces',\n",
       " 'changes',\n",
       " 'gp',\n",
       " 'however',\n",
       " 'replacement',\n",
       " 'true',\n",
       " 'feature',\n",
       " 'vector',\n",
       " 'approximation',\n",
       " 'leaves',\n",
       " 'mean',\n",
       " 'function',\n",
       " 'unchanged',\n",
       " 'bv',\n",
       " 'functions',\n",
       " 'qt',\n",
       " 'ko',\n",
       " 'xix',\n",
       " 'value',\n",
       " 'xl',\n",
       " 'change',\n",
       " 'xt',\n",
       " 'ct',\n",
       " 'xt',\n",
       " 'xt',\n",
       " 'ibl',\n",
       " 'yt',\n",
       " 'bi',\n",
       " 'factor',\n",
       " 'consequence',\n",
       " 'good',\n",
       " 'approximation',\n",
       " 'full',\n",
       " 'gp',\n",
       " 'solution',\n",
       " 'obtained',\n",
       " 'input',\n",
       " 'small',\n",
       " 'change',\n",
       " 'mean',\n",
       " 'function',\n",
       " 'posterior',\n",
       " 'process',\n",
       " 'included',\n",
       " 'set',\n",
       " 'bv',\n",
       " 'change',\n",
       " 'given',\n",
       " 'ct',\n",
       " 'decision',\n",
       " 'including',\n",
       " 'xt',\n",
       " 'based',\n",
       " 'score',\n",
       " 'associated',\n",
       " 'absence',\n",
       " 'matrix',\n",
       " 'inversions',\n",
       " 'important',\n",
       " 'issue',\n",
       " 'dealing',\n",
       " 'large',\n",
       " 'datasets',\n",
       " 'matrix',\n",
       " 'inversion',\n",
       " 'projection',\n",
       " 'equation',\n",
       " 'avoided',\n",
       " 'iterative',\n",
       " 'inversion',\n",
       " 'gram',\n",
       " 'matrix',\n",
       " 'ki',\n",
       " 'qt',\n",
       " 'yt',\n",
       " 'et',\n",
       " 'et',\n",
       " 'et',\n",
       " 'et',\n",
       " 'important',\n",
       " 'comment',\n",
       " 'new',\n",
       " 'input',\n",
       " 'linear',\n",
       " 'span',\n",
       " 'bvs',\n",
       " 'included',\n",
       " 'basis',\n",
       " 'set',\n",
       " 'avoiding',\n",
       " 'thus',\n",
       " 'small',\n",
       " 'singular',\n",
       " 'values',\n",
       " 'matrix',\n",
       " 'band',\n",
       " 'redundancy',\n",
       " 'representing',\n",
       " 'problem',\n",
       " 'deleting',\n",
       " 'basis',\n",
       " 'vector',\n",
       " 'section',\n",
       " 'gave',\n",
       " 'method',\n",
       " 'leave',\n",
       " 'vector',\n",
       " 'significant',\n",
       " 'prediction',\n",
       " 'purposes',\n",
       " 'however',\n",
       " 'provide',\n",
       " 'us',\n",
       " 'method',\n",
       " 'eliminate',\n",
       " 'one',\n",
       " 'already',\n",
       " 'existing',\n",
       " 'bv',\n",
       " 'let',\n",
       " 'us',\n",
       " 'assume',\n",
       " 'input',\n",
       " 'xt',\n",
       " 'added',\n",
       " 'set',\n",
       " 'bv',\n",
       " 'since',\n",
       " 'know',\n",
       " 'addition',\n",
       " 'taken',\n",
       " 'place',\n",
       " 'update',\n",
       " 'rule',\n",
       " 'th',\n",
       " 'unit',\n",
       " 'vector',\n",
       " 'et',\n",
       " 'last',\n",
       " 'performed',\n",
       " 'since',\n",
       " 'model',\n",
       " 'parameters',\n",
       " 'previous',\n",
       " 'step',\n",
       " 'empty',\n",
       " 'th',\n",
       " 'row',\n",
       " 'column',\n",
       " 'parameters',\n",
       " 'thefull',\n",
       " 'update',\n",
       " 'identified',\n",
       " 'removal',\n",
       " 'last',\n",
       " 'basis',\n",
       " 'vector',\n",
       " 'done',\n",
       " 'following',\n",
       " 'steps',\n",
       " 'computing',\n",
       " 'parameters',\n",
       " 'update',\n",
       " 'gp',\n",
       " 'performing',\n",
       " 'reduced',\n",
       " 'update',\n",
       " 'equation',\n",
       " 'also',\n",
       " 'minimises',\n",
       " 'kl',\n",
       " 'distance',\n",
       " 'full',\n",
       " 'posterior',\n",
       " 'one',\n",
       " 'increases',\n",
       " 'parameter',\n",
       " 'space',\n",
       " 'parametric',\n",
       " 'distribution',\n",
       " 'using',\n",
       " 'old',\n",
       " 'bvs',\n",
       " 'guide',\n",
       " 'available',\n",
       " 'sam',\n",
       " 'roweis',\n",
       " 'http',\n",
       " 'rvroweis',\n",
       " 'noteshtml',\n",
       " 'qt',\n",
       " 'dt',\n",
       " ...]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_word[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50542313",
   "metadata": {},
   "source": [
    "## create bigram and trigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be3ffd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(clean_word, min_count = 5, threshold =100)\n",
    "trigram = gensim.models.Phrases(bigram[clean_word], threshold= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51184f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93847803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigram(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigram(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19d0aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postage = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postage])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "13c994f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d8639658",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable = ['parser', 'ner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b6bca506",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words_bigram = make_bigram(clean_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e3192a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words_lemmatized = lemmatization(clean_words_bigram, allowed_postage=['NOUN', 'ADJ', 'VERB', 'ADV'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3c8e34e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sparse', 'representation', 'gaussian', 'process', 'model', 'manfre', 'opper', 'neural', 'compute', 'research']\n"
     ]
    }
   ],
   "source": [
    "print(clean_words_lemmatized[:1][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5111b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bde3568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(clean_words_lemmatized)\n",
    "\n",
    "texts = clean_words_lemmatized\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "717afe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 3), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:1][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b35bdaf",
   "metadata": {},
   "source": [
    "## training LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "09c2cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics  = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4dd55144",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus = corpus, id2word = id2word,\n",
    "                                       num_topics = num_topics,\n",
    "                                      chunksize =100,\n",
    "                                      passes =10,\n",
    "                                      per_word_topics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e52969bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.011*\"use\" + 0.010*\"model\" + 0.008*\"datum\" + 0.008*\"function\" + '\n",
      "  '0.008*\"set\" + 0.007*\"learn\" + 0.007*\"distribution\" + 0.005*\"problem\" + '\n",
      "  '0.005*\"result\" + 0.005*\"give\"'),\n",
      " (1,\n",
      "  '0.016*\"model\" + 0.011*\"network\" + 0.011*\"use\" + 0.008*\"datum\" + '\n",
      "  '0.007*\"state\" + 0.007*\"time\" + 0.006*\"distribution\" + 0.006*\"neural\" + '\n",
      "  '0.006*\"show\" + 0.005*\"set\"'),\n",
      " (2,\n",
      "  '0.010*\"set\" + 0.010*\"learn\" + 0.009*\"classifier\" + 0.008*\"method\" + '\n",
      "  '0.007*\"problem\" + 0.006*\"edge\" + 0.006*\"use\" + 0.006*\"example\" + '\n",
      "  '0.005*\"rate\" + 0.005*\"constraint\"'),\n",
      " (3,\n",
      "  '0.012*\"function\" + 0.012*\"policy\" + 0.011*\"use\" + 0.011*\"method\" + '\n",
      "  '0.009*\"problem\" + 0.007*\"state\" + 0.007*\"set\" + 0.006*\"time\" + '\n",
      "  '0.006*\"result\" + 0.006*\"show\"'),\n",
      " (4,\n",
      "  '0.011*\"object\" + 0.009*\"model\" + 0.009*\"network\" + 0.007*\"use\" + '\n",
      "  '0.007*\"circle\" + 0.006*\"node\" + 0.006*\"figure\" + 0.005*\"learn\" + '\n",
      "  '0.005*\"datum\" + 0.005*\"feature\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "35f63a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d1ef2c",
   "metadata": {},
   "source": [
    "## analyzing the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d89dd542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: gensim in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyLDAvis) (4.2.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyLDAvis) (1.7.2)\n",
      "Requirement already satisfied: sklearn in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyLDAvis) (1.3.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyLDAvis) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyLDAvis) (1.21.3)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyLDAvis) (1.0.1)\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.8.1-cp39-cp39-win_amd64.whl (88 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyLDAvis) (57.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim->pyLDAvis) (0.29.28)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from numexpr->pyLDAvis) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging->numexpr->pyLDAvis) (2.4.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->pyLDAvis) (3.0.0)\n",
      "Building wheels for collected packages: pyLDAvis, future\n",
      "  Building wheel for pyLDAvis (PEP 517): started\n",
      "  Building wheel for pyLDAvis (PEP 517): finished with status 'done'\n",
      "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136900 sha256=7da99b5810b780e2f8bfcefb071de55c693829c1fed5185a88094f568263ef07\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\57\\a4\\86\\d10c6c2e0bf149fbc0afb0aa5a6528ac35b30a133a0270c477\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=4a4576743a48c9957651dba8eb40e823cd7ebf5c64564e33e288e5303b2b14ab\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\2f\\a0\\d3\\4030d9f80e6b3be787f19fc911b8e7aa462986a40ab1e4bb94\n",
      "Successfully built pyLDAvis future\n",
      "Installing collected packages: numexpr, future, funcy, pyLDAvis\n",
      "Successfully installed funcy-1.17 future-0.18.2 numexpr-2.8.1 pyLDAvis-3.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9d0f49ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a1d33a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d7722779",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_data_filepath = os.path.join('content/results/ldavis_prepared '+str(num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b585af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 ==1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6d76a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "69c24533",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(LDAvis_prepared, 'content/results/ldavis_prepared'+ str(num_topics)+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9f28706a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1352423223027202565462810894\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1352423223027202565462810894_data = {\"mdsDat\": {\"x\": [0.025704460437604788, -0.0619323224127375, -0.039689887261973325, -0.054983633029654644, 0.13090138226676068], \"y\": [-0.04072951186717048, -0.036689665908255766, -0.037846283289364256, 0.09584349207191178, 0.01942196899287859], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [28.55997373919848, 27.450564656010478, 26.770950076543897, 8.916236713362158, 8.302274814884994]}, \"tinfo\": {\"Term\": [\"policy\", \"object\", \"classifier\", \"network\", \"model\", \"learn\", \"edge\", \"node\", \"set\", \"circle\", \"method\", \"image\", \"feature\", \"neuron\", \"constraint\", \"figure\", \"datum\", \"problem\", \"cluster\", \"rate\", \"principle\", \"example\", \"motion\", \"prediction\", \"iteration\", \"cost\", \"stimulus\", \"function\", \"state\", \"hypothesis\", \"subgraph\", \"dag\", \"seller\", \"food\", \"insect\", \"interventional\", \"memoize\", \"batch_normalization\", \"patch\", \"concentrator\", \"gy\", \"firing_rate\", \"triangulate\", \"arousal\", \"intervention\", \"separator\", \"buyer\", \"elbo\", \"minibatch\", \"timestep\", \"strong_annotation\", \"brain_state\", \"merge\", \"sleep\", \"nation\", \"img_bow\", \"lstm\", \"daquar\", \"sparseness\", \"degree_separability\", \"spike\", \"variational_inference\", \"animal\", \"weight_normalization\", \"artifact\", \"birth\", \"activity\", \"segmentation\", \"character\", \"generative\", \"adaptation\", \"network\", \"mcmc\", \"model\", \"neural\", \"dynamic\", \"price\", \"image\", \"component\", \"unit\", \"state\", \"latent\", \"mixture\", \"neuron\", \"datum\", \"distribution\", \"time\", \"use\", \"variable\", \"figure\", \"factor\", \"dataset\", \"bayesian\", \"show\", \"input\", \"parameter\", \"new\", \"system\", \"prior\", \"set\", \"number\", \"training\", \"result\", \"method\", \"weight\", \"give\", \"learn\", \"different\", \"base\", \"also\", \"function\", \"approach\", \"policy\", \"macro_action\", \"critic\", \"erm\", \"mdp\", \"valuation\", \"setpoint\", \"mdps\", \"pauc\", \"admm\", \"prec\", \"reward\", \"sinusoid\", \"voice\", \"log_det\", \"handicapped\", \"short_dot\", \"statistical_accuracy\", \"inverse_covariance\", \"markov_decision\", \"newton_direction\", \"coagent\", \"agd\", \"surrogate\", \"svrg\", \"actor\", \"sn\", \"dpps\", \"polyhedron\", \"pomdp\", \"isotonic_regression\", \"query\", \"ax\", \"reinforcement\", \"processor\", \"gradient\", \"vehicle\", \"trial\", \"action\", \"iterate\", \"convergence\", \"update\", \"function\", \"method\", \"dot_product\", \"solve\", \"linear\", \"problem\", \"iteration\", \"matrix\", \"loss\", \"state\", \"optimal\", \"step\", \"value\", \"log\", \"use\", \"size\", \"follow\", \"result\", \"time\", \"point\", \"number\", \"set\", \"show\", \"algorithm\", \"learn\", \"give\", \"parameter\", \"sample\", \"large\", \"also\", \"engine\", \"density_functional\", \"thalnet\", \"mc\", \"ofo\", \"reflex\", \"gru\", \"rddp\", \"mhmm\", \"ponss\", \"triangle_inequality\", \"pile\", \"nb_process\", \"hyperkernel\", \"stationary_ergodic\", \"decision_tree\", \"reading_mechanism\", \"refit\", \"information_theoretic\", \"poss\", \"subset_selection\", \"hilbert_space\", \"cost_go\", \"novelty\", \"nb\", \"ddp\", \"fdkm\", \"influence_maximization\", \"gamma_nb\", \"count_mixture\", \"conversion\", \"regularized_risk\", \"knowledge_partitioning\", \"perceptron\", \"pole\", \"learner\", \"read\", \"basis_vector\", \"beta\", \"kernel\", \"vibration\", \"context\", \"module\", \"time_serie\", \"sequence\", \"estimator\", \"process\", \"output\", \"datum\", \"distribution\", \"cluster\", \"online\", \"conditional\", \"bias\", \"hypothesis\", \"use\", \"solution\", \"assumption\", \"denote\", \"measure\", \"set\", \"model\", \"function\", \"mean\", \"learn\", \"let\", \"class\", \"give\", \"estimate\", \"case\", \"sample\", \"error\", \"probability\", \"result\", \"problem\", \"parameter\", \"show\", \"follow\", \"value\", \"vector\", \"number\", \"also\", \"method\", \"time\", \"training\", \"calibration\", \"mlfre\", \"lpboost\", \"protagonist\", \"softboost\", \"message_estimator\", \"equalized_odd\", \"antagonist\", \"generalized_fp\", \"graph_matche\", \"calibrated_classifier\", \"adaboost\", \"sdp\", \"meta\", \"inactive_node\", \"cap\", \"parity\", \"calibrate\", \"fairness\", \"rocch\", \"cnn_message\", \"pf\", \"brownboost\", \"dpf\", \"cf_ht\", \"nsdbn\", \"node_gij\", \"pd_pd\", \"recidivism\", \"disparate\", \"false_negative\", \"message\", \"roc_curve\", \"classifier\", \"cnn\", \"soft_margin\", \"game\", \"false_positive\", \"edge\", \"constraint\", \"relaxation\", \"potential\", \"boost\", \"cost\", \"normalization\", \"learn\", \"rate\", \"set\", \"example\", \"prediction\", \"method\", \"hypothesis\", \"group\", \"problem\", \"margin\", \"node\", \"training\", \"iteration\", \"base\", \"algorithm\", \"error\", \"learning\", \"function\", \"use\", \"show\", \"point\", \"result\", \"number\", \"network\", \"figure\", \"datum\", \"give\", \"see\", \"case\", \"infant\", \"twitter\", \"habituation\", \"eigenflow\", \"invisible\", \"eld\", \"occlusion_event\", \"protocol\", \"halflife\", \"object_perception\", \"excitation\", \"pe\", \"interconnect\", \"inertia\", \"friend\", \"ego_network\", \"codebook\", \"contact\", \"visible_invisible\", \"velocity_eld\", \"row_parallel\", \"occluder\", \"uniform_newscast\", \"facebook_google\", \"disinhibition\", \"edge_filtere\", \"education\", \"spelke\", \"objects_tend\", \"halflive\", \"circle\", \"visible\", \"object\", \"facebook\", \"perception\", \"profile\", \"scene\", \"contextual\", \"pixel\", \"principle\", \"chip\", \"motion\", \"car\", \"node\", \"stimulus\", \"visual\", \"eigenvector\", \"user\", \"neuron\", \"representation\", \"network\", \"feature\", \"edge\", \"model\", \"image\", \"information\", \"figure\", \"cluster\", \"location\", \"use\", \"step\", \"learn\", \"datum\", \"local\", \"show\", \"consider\", \"time\", \"set\", \"give\", \"result\"], \"Freq\": [594.0, 225.0, 228.0, 999.0, 1657.0, 1060.0, 261.0, 285.0, 1242.0, 127.0, 1166.0, 389.0, 370.0, 207.0, 207.0, 726.0, 1034.0, 997.0, 277.0, 380.0, 112.0, 481.0, 105.0, 272.0, 439.0, 189.0, 110.0, 1280.0, 818.0, 156.0, 48.181331540105624, 42.37101248932398, 45.146208011449765, 35.71637271624647, 29.96007915260017, 31.831563269212158, 29.931663370125978, 31.7821559255403, 28.96068635895085, 28.02247161821554, 28.017069378201434, 46.70148204156759, 26.120534302895468, 26.064320136805843, 25.113547503283463, 23.240078449340576, 26.02519557112655, 22.248966517884302, 22.226307979768002, 21.297676812817716, 20.344676912912377, 23.11555373002117, 52.741896386024266, 18.456430417285194, 20.296536780398117, 20.292645624090813, 20.28579095326149, 20.2820417986917, 45.22383048296671, 18.413723394727977, 83.83627615690028, 51.62395992449972, 59.406297235058695, 53.2874979314945, 37.68049135762603, 22.904434190269956, 75.19017314698895, 119.0146444206335, 48.0212369290362, 49.164085434511506, 98.71185956378478, 583.9188497937245, 58.050013944111846, 836.7034774401449, 305.6262154261678, 137.30581414189865, 61.49589602049159, 229.06445984451207, 152.43829721391103, 128.8800607846769, 392.32227692283186, 93.34661408929662, 108.87248519214182, 124.46457232258848, 399.09126513853937, 340.1106335466492, 350.8861946123146, 573.8509512999638, 231.7965485783196, 279.56647638889393, 137.513099634737, 139.7246099988196, 102.0476779456143, 296.98359454581987, 180.621357362283, 245.18736181776177, 152.51380509433673, 169.69054517959506, 139.99332667810407, 284.3187682764103, 214.7545619679731, 179.30464316419375, 228.04654199810582, 249.44232657657398, 154.22304913884514, 196.24290676626944, 213.1265305693453, 154.95049317018442, 166.18373200984482, 164.30013452762407, 178.91469404500418, 151.67757538408486, 587.5517870357976, 78.34926927275845, 39.19762950897664, 34.45124450428286, 39.9643853762665, 29.72722831823578, 27.815360433478382, 40.78398791140571, 25.89358546748192, 24.92916567534469, 23.012620188438927, 131.7045777698132, 21.155119600202656, 19.257708814378276, 20.17102213128271, 19.237221124664064, 18.291528060062138, 19.203552180560948, 18.283267708409078, 35.67487436865948, 17.335499171112716, 18.237895115923962, 17.31728426493523, 19.13908642611914, 17.300643394875536, 17.293753834337316, 18.175702145018878, 16.33247766381518, 15.404717632758901, 14.478491582235893, 35.996746210496916, 56.74353028081553, 28.975939840733062, 101.1438520792287, 66.12146241291143, 282.18263239268765, 25.848025948935685, 61.53719274356959, 127.97105311080831, 38.16659282915462, 165.89969203124457, 204.97578399553277, 598.3585582424098, 551.4638772820032, 42.13208382876155, 168.8971790056694, 216.47390865687765, 444.4448500895368, 230.44078657692359, 238.68378674418184, 204.3391909754656, 334.53121053202597, 200.49997891492268, 229.58992288337322, 275.37147398524485, 189.17929752594114, 567.7715204834132, 169.31731758112667, 238.39427091339923, 302.1411848682506, 303.0665341465346, 218.61471646029523, 255.22808427312341, 333.67752217799654, 285.5277662358214, 188.3860189275354, 264.2447090986474, 230.8913080550128, 206.53615967546693, 183.18282816860253, 179.37231370906116, 173.24227826335544, 38.53281113547911, 27.087502853963574, 22.245237566250808, 20.36025692035853, 21.267708373746025, 21.26789067193598, 20.32832580089535, 24.904513859065954, 22.134957289003776, 18.43071738832094, 23.031731276979478, 20.236997301348737, 18.387519951356186, 18.370812177565483, 16.503902201455496, 50.41564664738075, 15.52989599381219, 14.600305438022087, 18.23425781198808, 14.57978097673053, 40.10826681655604, 14.548349007073186, 15.427868326688314, 13.608947625679104, 13.602739359543332, 14.477213649091938, 13.568859385620845, 12.659673205806692, 12.639514787706082, 12.637383635056818, 80.7263146064067, 13.522778454270492, 16.10427997328126, 43.24785543557809, 18.414808254219576, 38.17804438971162, 31.69459768938712, 22.271186122099113, 35.297819797046, 159.256659429342, 18.40665402724656, 123.58641311107914, 106.04250986359368, 59.34201930612774, 157.81804313561565, 81.93672828977108, 222.83769616363682, 201.62638159027304, 401.8427590217769, 336.6391295030018, 141.2518716341794, 133.52043756800484, 89.01911714001177, 54.66362272945433, 89.44053268275212, 565.3407392024479, 199.1271486401302, 125.08227682369623, 129.85593143867754, 146.82378398038577, 392.03407469812413, 473.8143159067381, 394.0672956022945, 164.12554893365473, 337.4682795433695, 157.6396223992224, 151.52611659876277, 248.08954884063502, 190.7299847332076, 189.77602698602652, 177.23878437650862, 173.56262273751233, 175.88547206119412, 248.6242438998905, 259.23786443448927, 214.733170043171, 240.8787815539554, 192.92007467664737, 195.1514276707029, 160.79171435917786, 186.54164041460453, 172.35769536157903, 194.90586332796406, 170.47232697033036, 159.19629847311163, 48.358203187097494, 39.607426336935646, 29.157536453651524, 24.746244881092583, 23.00928387439011, 23.8116142693035, 18.629903640728795, 19.46532306408236, 13.362054948632386, 18.36766822345647, 12.482427835420982, 12.475445543493182, 14.085258484133762, 13.250810510091895, 11.561833124202838, 10.718801123356153, 10.706622348602309, 29.631164252593283, 22.15392661040131, 10.631143242806132, 9.805638770363986, 9.774563957262542, 8.95748931332177, 8.921051290478683, 8.089908942566886, 11.256635168372975, 8.043530911588075, 8.013733535056721, 7.209302188258917, 7.208325438380542, 22.16337511144481, 44.8504115163412, 40.13449155793573, 156.6233955815452, 22.069341131047402, 20.386540350950654, 39.628763527174634, 23.089139320785893, 98.84923875784905, 82.6443154614359, 27.262705296404846, 44.95866643527368, 26.499435259232477, 65.14113496901088, 29.70816703037536, 161.03342080053713, 88.66516554743474, 173.1136551285653, 92.93529435562756, 66.8574391854883, 138.7436551221961, 49.58339351702267, 52.51881265026961, 110.9708522006063, 34.443232635632114, 61.32697825401552, 74.56944374467294, 68.83226456760386, 76.40759546053138, 66.05125961542521, 64.68258005423071, 64.76771293756185, 82.5994591749642, 93.03617088574028, 73.21140386641605, 62.793350150356616, 69.5302354782255, 64.93341443444218, 67.6085082819007, 62.226625125710875, 65.24649866975379, 61.63088383531465, 54.607783326509185, 54.718247136138494, 41.061685360029706, 18.17522879970122, 20.645468284829782, 17.326568223967765, 16.492238765312727, 17.238128091685596, 14.769397995343875, 15.583570562839528, 13.89288282511361, 14.674982226954931, 13.037432691971558, 12.196424950705525, 13.822851492815575, 12.99550004082314, 27.62365980717691, 12.179484260147833, 12.170857771201383, 12.161881305068922, 11.347202616719654, 12.954042999696735, 11.335562734463917, 11.32743428997448, 11.293298951054888, 10.462807575539761, 9.6208001558501, 9.620431333738688, 9.61115604093202, 10.408063964628571, 9.552192064108507, 8.738811880857076, 105.17288972774664, 26.6732581713381, 166.23418111550455, 14.764625742330535, 20.814424706951893, 26.6216230757406, 48.88744445243706, 34.112520407063826, 40.606262740006876, 70.46555527435913, 20.238938375069974, 60.52704313691681, 20.772791779855083, 96.3799695791776, 50.43010081425467, 47.131772232910855, 28.467146104821815, 52.294707471124845, 64.14689845118036, 59.768077227070755, 131.63245721357134, 76.84634650871571, 61.71267969488885, 144.66888125334526, 72.10802871913646, 64.46650838597365, 93.01279420370376, 53.26467552379383, 37.81295303283053, 109.34797756597354, 65.23815244372727, 84.76690813917142, 80.11559288327949, 44.922953142448286, 66.73690910313562, 53.4979649317238, 56.532931193707974, 59.40560854765223, 54.645976250067555, 48.79301881289671], \"Total\": [594.0, 225.0, 228.0, 999.0, 1657.0, 1060.0, 261.0, 285.0, 1242.0, 127.0, 1166.0, 389.0, 370.0, 207.0, 207.0, 726.0, 1034.0, 997.0, 277.0, 380.0, 112.0, 481.0, 105.0, 272.0, 439.0, 189.0, 110.0, 1280.0, 818.0, 156.0, 49.16685426447431, 43.37957283311729, 46.26715407491499, 36.624846250064394, 30.83592619680259, 32.76399308454585, 30.834209991159003, 32.75901469063964, 29.8683847490315, 28.90518739835797, 28.904147140058203, 48.19773726227706, 26.97505614514649, 26.969955468406166, 26.00956381633181, 24.079840826170134, 26.971684547022132, 23.11299556849414, 23.112980530377154, 22.149288674608858, 21.1844620856365, 24.079299183607905, 54.951529013535996, 19.255904651784714, 21.182678384446238, 21.182231837910365, 21.181787117628527, 21.18057295221682, 47.232630078192855, 19.253405779043405, 87.7458286147401, 53.98523178563331, 62.56256868618981, 56.882901265697825, 40.46688091171807, 24.072436010963905, 87.82157235423279, 146.1996455932506, 54.84747334881553, 56.472179250769514, 125.30881231459715, 999.0448588709723, 69.40702180017942, 1657.5621689402813, 515.1883249928204, 199.79329332671202, 76.07866539508069, 389.9666716941217, 235.08975908610836, 192.06745157583268, 818.2353517917882, 135.77670378449506, 166.4611117806433, 207.53173196534323, 1034.4703315073368, 863.8315559784169, 915.7173761950274, 1909.3473594375387, 539.8274756345154, 726.7639784620068, 251.8962097017137, 261.09795134782684, 167.87540251483898, 963.3384553051484, 427.3592021158872, 744.4846745811165, 336.2759074812675, 407.6726884521974, 303.14533442574424, 1242.5496288287486, 766.094545190896, 528.9278190441629, 897.1352250573691, 1166.1500545614344, 388.10772522842836, 791.5006237472994, 1060.6398481510707, 422.83961946098646, 581.2676372278142, 584.9862524642965, 1280.678915571842, 476.021976081606, 594.948716015799, 79.492479789785, 40.204118915757775, 35.41330688408993, 41.154015263369836, 30.626848452515784, 28.712307220608434, 42.113775645680626, 26.787298063639163, 25.834917354099602, 23.910241406512082, 136.84724508172116, 22.003205584766974, 20.086684162290457, 21.04368436436728, 20.087147428394626, 19.1271198987239, 20.082669430938626, 19.127275296065044, 37.32750888247579, 18.169351115964773, 19.123986825540594, 18.167208265432155, 20.079724829409347, 18.166844595228905, 18.167092327233245, 19.124195593430848, 17.210503405654176, 16.253330911680443, 15.293551344501761, 38.22051759264316, 62.16920034868973, 31.585856143916757, 124.8372968169585, 78.38010065112465, 406.3510346951574, 28.518132811099985, 78.32155819512614, 192.19973768163217, 45.81463438429909, 263.5978912411025, 339.09704077800876, 1280.678915571842, 1166.1500545614344, 51.765625661376134, 287.4523898124448, 391.47785218511746, 997.4464949828548, 439.7561684792751, 469.85536005789174, 396.63645888639934, 818.2353517917882, 423.53766300129206, 513.1865377398237, 675.4347811509571, 404.6199833349856, 1909.3473594375387, 347.13273344798966, 623.334908331007, 897.1352250573691, 915.7173761950274, 561.6134202720363, 766.094545190896, 1242.5496288287486, 963.3384553051484, 493.65909833261725, 1060.6398481510707, 791.5006237472994, 744.4846745811165, 486.694363980936, 511.065413541243, 584.9862524642965, 39.55008020612158, 27.960290551546194, 23.128914690822388, 21.2020075398403, 22.162782127369752, 22.16598973915936, 21.1976792946195, 26.022391886834555, 23.131468504990167, 19.269262857205465, 24.095246598259674, 21.200337886002714, 19.263985503174677, 19.2697055579205, 17.338297719797346, 52.98221492950391, 16.369846388059308, 15.407840134166687, 19.263922002713628, 15.406585854976061, 42.4310574642965, 15.407204965695916, 16.368019196448994, 14.440726422279203, 14.437691464340585, 15.403121110426422, 14.441421645384072, 13.475180432486637, 13.471687428834583, 13.471118076394161, 86.834525380349, 14.435987995966858, 17.334583718813317, 49.19318820090313, 20.220557637161132, 45.74478430274789, 37.17563009555762, 25.05732849450628, 42.42552975936638, 241.58894644548332, 20.22015383947393, 199.60810420911554, 169.64334905757033, 83.60541230235394, 286.0514888207498, 131.76013781269756, 476.24616929428294, 422.2926315633896, 1034.4703315073368, 863.8315559784169, 277.51679577313496, 259.0157409929242, 154.5673820722629, 79.9834973513658, 156.10142112785192, 1909.3473594375387, 463.1516599023428, 245.73999201048053, 260.9477094064098, 314.6099156078661, 1242.5496288287486, 1657.5621689402813, 1280.678915571842, 377.18716253366296, 1060.6398481510707, 369.9989340516233, 359.8087286524972, 791.5006237472994, 523.532984809766, 535.527730900341, 486.694363980936, 473.06142273703387, 491.4150511530213, 897.1352250573691, 997.4464949828548, 744.4846745811165, 963.3384553051484, 623.334908331007, 675.4347811509571, 450.75703187898387, 766.094545190896, 584.9862524642965, 1166.1500545614344, 915.7173761950274, 528.9278190441629, 49.64492811383739, 40.78731363326973, 30.14984335495475, 25.72131466318028, 23.95055700060947, 24.842950906370326, 19.522323004754877, 20.40949839053793, 14.208775755012184, 19.54282901247178, 13.323133442072622, 13.323012574182503, 15.106281147811629, 14.217352315790082, 12.441482815543917, 11.551955490065769, 11.552929613011399, 32.03277081148642, 24.023297213578854, 11.55973867177133, 10.669219020068926, 10.67223209173393, 9.781112862676009, 9.784216400987512, 8.89504155138466, 12.44276993433866, 8.899094333256013, 8.901350612791608, 8.009799590474682, 8.009957209098625, 24.790597694318524, 50.896553902034, 46.48898050356485, 228.83159985109876, 26.923513467542513, 25.14915460021433, 58.0319074101767, 31.322507163784326, 261.70567603825737, 207.80486293933006, 45.391942137572684, 97.91712053699204, 45.723777829100634, 189.6432957040667, 55.060941098464525, 1060.6398481510707, 380.5795327452042, 1242.5496288287486, 481.58516742779886, 272.99653792425084, 1166.1500545614344, 156.10142112785192, 183.9215976950021, 997.4464949828548, 76.87396080700589, 285.8914349357547, 528.9278190441629, 439.7561684792751, 581.2676372278142, 493.65909833261725, 473.06142273703387, 497.3479774015026, 1280.678915571842, 1909.3473594375387, 963.3384553051484, 561.6134202720363, 897.1352250573691, 766.094545190896, 999.0448588709723, 726.7639784620068, 1034.4703315073368, 791.5006237472994, 438.09276472272825, 535.527730900341, 42.45072874139432, 19.084211641625874, 21.684864681046868, 18.21881739597169, 17.351078420222148, 18.22504327635665, 15.621973610052935, 16.491331359380414, 14.760621469437366, 15.630240865353883, 13.893878891227377, 13.029409066681042, 14.767185392491335, 13.898311337289256, 29.55359278852502, 13.030753518162168, 13.031871233235089, 13.03175384685057, 12.163227670941039, 13.90162692986551, 12.164921066219406, 12.165005892680199, 12.16860603334168, 11.301437018464972, 10.43505548230795, 10.435552041077615, 10.435947279253801, 11.30526048427849, 10.439474646889511, 9.573163371522297, 127.54639346723185, 30.615049037602073, 225.30645272558735, 16.58290175719069, 24.54173836451584, 32.46322105472657, 67.56870172850084, 44.95226388199967, 56.82595967667713, 112.19741250052556, 24.63141661283793, 105.74887625460293, 26.392476739079214, 285.8914349357547, 110.84378450397115, 106.30935140382482, 48.33525761226639, 131.39181021679622, 207.53173196534323, 205.97561704405692, 999.0448588709723, 370.1067633312333, 261.70567603825737, 1657.5621689402813, 389.9666716941217, 319.67601275265713, 726.7639784620068, 277.51679577313496, 119.530558862583, 1909.3473594375387, 513.1865377398237, 1060.6398481510707, 1034.4703315073368, 201.35974093985516, 963.3384553051484, 436.6056336060862, 915.7173761950274, 1242.5496288287486, 791.5006237472994, 897.1352250573691], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -7.0037, -7.1322, -7.0688, -7.3031, -7.4788, -7.4182, -7.4798, -7.4198, -7.5128, -7.5457, -7.5459, -7.0349, -7.616, -7.6181, -7.6553, -7.7328, -7.6196, -7.7764, -7.7774, -7.8201, -7.8659, -7.7382, -6.9133, -7.9633, -7.8682, -7.8684, -7.8688, -7.869, -7.0671, -7.9656, -6.4498, -6.9347, -6.7943, -6.903, -7.2495, -7.7474, -6.5587, -6.0994, -7.007, -6.9835, -6.2865, -4.5089, -6.8174, -4.1492, -5.1563, -5.9565, -6.7597, -5.4447, -5.8519, -6.0198, -4.9066, -6.3424, -6.1885, -6.0547, -4.8895, -5.0494, -5.0182, -4.5263, -5.4328, -5.2455, -5.955, -5.939, -6.2533, -5.185, -5.6823, -5.3767, -5.8514, -5.7447, -5.9371, -5.2286, -5.5092, -5.6896, -5.4491, -5.3595, -5.8403, -5.5993, -5.5168, -5.8356, -5.7656, -5.777, -5.6918, -5.8569, -4.4631, -6.4779, -7.1705, -7.2995, -7.1511, -7.447, -7.5135, -7.1308, -7.5851, -7.623, -7.703, -5.9585, -7.7872, -7.8812, -7.8348, -7.8822, -7.9326, -7.884, -7.9331, -7.2646, -7.9863, -7.9356, -7.9874, -7.8873, -7.9883, -7.9887, -7.939, -8.0459, -8.1044, -8.1664, -7.2556, -6.8005, -7.4726, -6.2225, -6.6476, -5.1965, -7.5868, -6.7194, -5.9873, -7.1971, -5.7277, -5.5162, -4.4449, -4.5265, -7.0983, -5.7098, -5.4616, -4.7422, -5.3991, -5.3639, -5.5193, -5.0263, -5.5383, -5.4028, -5.221, -5.5964, -4.4974, -5.7073, -5.3651, -5.1282, -5.1251, -5.4518, -5.2969, -5.0289, -5.1847, -5.6006, -5.2622, -5.3971, -5.5086, -5.6286, -5.6496, -5.6844, -7.1625, -7.5149, -7.7119, -7.8004, -7.7568, -7.7568, -7.802, -7.599, -7.7168, -7.9, -7.6771, -7.8065, -7.9023, -7.9032, -8.0104, -6.8937, -8.0712, -8.133, -7.9107, -8.1344, -7.1224, -8.1365, -8.0778, -8.2033, -8.2037, -8.1414, -8.2062, -8.2756, -8.2772, -8.2773, -6.4229, -8.2096, -8.0349, -7.0471, -7.9008, -7.1717, -7.3579, -7.7107, -7.2502, -5.7435, -7.9013, -5.9971, -6.1502, -6.7307, -5.7526, -6.4081, -5.4076, -5.5076, -4.8179, -4.995, -5.8635, -5.9197, -6.3252, -6.8128, -6.3204, -4.4766, -5.5201, -5.985, -5.9476, -5.8248, -4.8427, -4.6532, -4.8375, -5.7134, -4.9925, -5.7537, -5.7932, -5.3002, -5.5631, -5.5682, -5.6365, -5.6575, -5.6442, -5.2981, -5.2563, -5.4446, -5.3297, -5.5517, -5.5402, -5.7339, -5.5853, -5.6644, -5.5415, -5.6754, -5.7439, -5.8359, -6.0355, -6.3418, -6.5059, -6.5787, -6.5444, -6.7898, -6.7459, -7.1221, -6.804, -7.1902, -7.1908, -7.0694, -7.1305, -7.2668, -7.3426, -7.3437, -6.3257, -6.6165, -7.3508, -7.4316, -7.4348, -7.5221, -7.5261, -7.6239, -7.2936, -7.6297, -7.6334, -7.7392, -7.7393, -6.6161, -5.9112, -6.0223, -4.6607, -6.6204, -6.6997, -6.035, -6.5752, -5.121, -5.3, -6.409, -5.9088, -6.4374, -5.538, -6.3231, -4.6329, -5.2297, -4.5606, -5.1827, -5.512, -4.7819, -5.8109, -5.7534, -5.0053, -6.1752, -5.5983, -5.4028, -5.4829, -5.3785, -5.5241, -5.5451, -5.5438, -5.3006, -5.1816, -5.4212, -5.5747, -5.4728, -5.5412, -5.5008, -5.5838, -5.5364, -5.5934, -5.7144, -5.7124, -5.9281, -6.7432, -6.6157, -6.791, -6.8403, -6.7961, -6.9507, -6.897, -7.0118, -6.9571, -7.0754, -7.1421, -7.0169, -7.0786, -6.3245, -7.1435, -7.1442, -7.1449, -7.2142, -7.0818, -7.2153, -7.216, -7.219, -7.2954, -7.3793, -7.3793, -7.3803, -7.3006, -7.3864, -7.4754, -4.9876, -6.3596, -4.5298, -6.951, -6.6076, -6.3615, -5.7537, -6.1136, -5.9393, -5.3881, -6.6356, -5.5401, -6.6096, -5.0749, -5.7226, -5.7903, -6.2945, -5.6863, -5.482, -5.5527, -4.7632, -5.3014, -5.5207, -4.6688, -5.365, -5.4771, -5.1105, -5.6679, -6.0106, -4.9487, -5.4652, -5.2033, -5.2597, -5.8383, -5.4425, -5.6636, -5.6084, -5.5588, -5.6423, -5.7556], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2329, 1.2296, 1.2286, 1.228, 1.2243, 1.2243, 1.2235, 1.2229, 1.2223, 1.2221, 1.222, 1.2216, 1.221, 1.219, 1.2181, 1.2177, 1.2174, 1.2151, 1.214, 1.214, 1.2127, 1.2123, 1.2121, 1.2108, 1.2104, 1.2103, 1.2099, 1.2098, 1.2097, 1.2086, 1.2076, 1.2084, 1.2014, 1.1879, 1.1818, 1.2034, 1.0979, 1.0474, 1.1203, 1.1146, 1.0146, 0.7161, 1.0745, 0.5695, 0.731, 0.8781, 1.0404, 0.7211, 0.82, 0.8542, 0.5181, 0.8785, 0.8286, 0.7419, 0.3007, 0.3211, 0.2939, 0.051, 0.4078, 0.2978, 0.6479, 0.6279, 0.7554, 0.0764, 0.3919, 0.1425, 0.4625, 0.3767, 0.4805, -0.2217, -0.0186, 0.1714, -0.1165, -0.2891, 0.3303, -0.1414, -0.3516, 0.2493, 0.001, -0.0167, -0.7151, 0.1095, 1.2803, 1.2783, 1.2674, 1.2652, 1.2635, 1.263, 1.261, 1.2607, 1.2589, 1.2571, 1.2545, 1.2545, 1.2535, 1.2506, 1.2504, 1.2496, 1.2481, 1.248, 1.2477, 1.2475, 1.2458, 1.2453, 1.2449, 1.2448, 1.2439, 1.2435, 1.2419, 1.2404, 1.2392, 1.238, 1.2328, 1.2015, 1.2065, 1.0823, 1.1227, 0.9281, 1.1945, 1.0516, 0.8861, 1.1101, 0.8297, 0.7894, 0.5318, 0.5439, 1.0869, 0.761, 0.7003, 0.4844, 0.6466, 0.6155, 0.6295, 0.3984, 0.545, 0.4884, 0.3955, 0.5325, 0.08, 0.5749, 0.3316, 0.2045, 0.187, 0.3493, 0.1936, -0.022, 0.0767, 0.3294, -0.097, 0.0608, 0.0106, 0.3156, 0.2457, 0.0759, 1.2918, 1.2861, 1.2789, 1.2773, 1.2766, 1.2765, 1.276, 1.2739, 1.2738, 1.2734, 1.2727, 1.2713, 1.2713, 1.2701, 1.2685, 1.2682, 1.2652, 1.264, 1.2629, 1.2627, 1.2616, 1.2605, 1.2587, 1.2585, 1.2583, 1.2559, 1.2555, 1.2554, 1.2541, 1.254, 1.2449, 1.2525, 1.2442, 1.189, 1.2243, 1.137, 1.1583, 1.2, 1.1339, 0.9011, 1.2239, 0.8384, 0.848, 0.9751, 0.7231, 0.8428, 0.5584, 0.5786, 0.3723, 0.3755, 0.6425, 0.6552, 0.7661, 0.9372, 0.7609, 0.1008, 0.4737, 0.6426, 0.62, 0.5558, 0.1643, 0.0656, 0.1392, 0.4857, 0.1727, 0.4647, 0.453, 0.1577, 0.3081, 0.2804, 0.3077, 0.3152, 0.2904, 0.0346, -0.0296, 0.0746, -0.0683, 0.145, 0.0763, 0.287, -0.0948, 0.0958, -0.4711, -0.3633, 0.1171, 2.391, 2.3879, 2.3838, 2.3786, 2.3772, 2.3749, 2.3705, 2.3699, 2.3559, 2.3553, 2.3521, 2.3516, 2.3473, 2.3469, 2.344, 2.3424, 2.3412, 2.3394, 2.3363, 2.3336, 2.3329, 2.3294, 2.3293, 2.3249, 2.3224, 2.3171, 2.3162, 2.3122, 2.312, 2.3118, 2.3053, 2.2908, 2.2703, 2.0382, 2.2185, 2.2073, 2.0359, 2.1123, 1.4437, 1.4952, 1.9075, 1.6389, 1.8718, 1.3487, 1.8003, 0.5323, 0.9605, 0.4463, 0.7721, 1.0104, 0.2885, 1.2704, 1.164, 0.2214, 1.6144, 0.8779, 0.4582, 0.5627, 0.3882, 0.4059, 0.4276, 0.3788, -0.3238, -0.6042, -0.1598, 0.2263, -0.1401, -0.0506, -0.2758, -0.0405, -0.3462, -0.1355, 0.335, 0.1362, 2.4554, 2.4398, 2.4395, 2.4384, 2.4379, 2.433, 2.4325, 2.432, 2.4281, 2.4256, 2.425, 2.4226, 2.4226, 2.4215, 2.4211, 2.4211, 2.4203, 2.4196, 2.4192, 2.418, 2.418, 2.4173, 2.414, 2.4115, 2.4074, 2.4073, 2.4063, 2.406, 2.3998, 2.3975, 2.2958, 2.3508, 2.1846, 2.3725, 2.3239, 2.2903, 2.165, 2.2127, 2.1526, 2.0235, 2.2922, 1.9307, 2.2492, 1.4013, 1.7011, 1.6752, 1.9592, 1.5674, 1.3145, 1.2514, 0.4619, 0.9167, 1.0439, 0.05, 0.8007, 0.8875, 0.4328, 0.838, 1.3377, -0.3713, 0.426, -0.0381, -0.0695, 0.9885, -0.181, 0.3893, -0.2962, -0.5519, -0.1844, -0.423]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 1, 2, 3, 5, 2, 4, 1, 2, 3, 4, 2, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 4, 1, 2, 3, 4, 5, 1, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 3, 1, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 1, 2, 1, 3, 4, 5, 1, 4, 1, 3, 4, 4, 4, 4, 1, 2, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 3, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 4, 2, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 3, 3, 2, 1, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 3, 4, 1, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 5, 4, 1, 2, 3, 4, 5, 2, 3, 4, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 5, 5, 5, 1, 2, 3, 4, 5, 1, 5, 3, 4, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 5, 3, 5, 5, 1, 2, 3, 4, 5, 3, 4, 4, 5, 1, 2, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 1, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 3, 4, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 4, 1, 2, 3, 4, 5, 3, 1, 5, 5, 5, 2, 3, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 5, 5, 3, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 5, 1, 1, 2, 5, 1, 2, 4, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 3, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 4, 1, 2, 1, 3, 4, 5, 2, 3, 1, 2, 3, 4, 5, 3, 1, 2, 3, 2, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 3, 1, 2, 4, 4, 4, 1, 2, 3, 4, 5, 3, 1, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 5, 1, 3, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 5, 5, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 4, 5, 2, 3, 5, 1, 3, 4, 3, 1, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 1, 2, 3, 4, 5, 2, 2, 3, 3, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 5, 1, 2, 4, 5, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 3, 2, 3, 5, 3, 4, 3, 3, 3, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 4, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 2, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 5, 1, 2, 3, 1, 2, 3, 4, 5, 3, 2, 1, 2, 3, 4, 5, 1, 3, 5, 1, 1, 1, 2, 3, 2, 2, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 3, 1, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 2, 5, 5, 2, 3, 1, 2, 5, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 3], \"Freq\": [0.04162336586146408, 0.6659738537834253, 0.04162336586146408, 0.18210222564390535, 0.06763796952487913, 0.854004295180274, 0.034160171807210954, 0.011386723935736986, 0.09109379148589589, 0.9357578909045492, 0.9006971909081395, 0.7900481871255239, 0.01596056943687927, 0.18354654852411162, 0.007980284718439635, 0.9676826001548205, 0.9357519191513276, 0.22282583339704656, 0.3808296061694978, 0.2289029015806024, 0.13369550003822794, 0.03443671970681629, 0.28034846854800133, 0.2957334454805136, 0.29402400359912334, 0.0854720940695126, 0.0427360470347563, 0.9430559076936331, 0.015983998435485307, 0.015983998435485307, 0.015983998435485307, 0.9309390969063998, 0.3193129889741522, 0.23108176833655752, 0.2941040687919823, 0.07772750389502389, 0.07982824724353806, 0.9640357037466222, 0.9390395094423071, 0.049423132075910904, 0.1342882765235566, 0.27264589475994827, 0.508667714104381, 0.06917880911819582, 0.012208025138505146, 0.0316597402154823, 0.9181324662489867, 0.0316597402154823, 0.28558273223620084, 0.25805668575560314, 0.2597770636606405, 0.13074872078283895, 0.06537436039141947, 0.0798169685343149, 0.877986653877464, 0.9768303565352191, 0.6075934798785303, 0.14296317173612477, 0.20253115995951013, 0.02382719528935413, 0.02382719528935413, 0.14142428000384288, 0.8249749666890835, 0.13752836977954633, 0.05001031628347139, 0.6876418488977316, 0.10002063256694278, 0.037507737212603545, 0.9554496266819253, 0.04154128811660545, 0.1312227529060675, 0.262445505812135, 0.5686319292596258, 0.021870458817677917, 0.955177300826818, 0.9201406962947256, 0.9639739021369574, 0.06243605998900454, 0.9365408998350682, 0.900689019754591, 0.9668661396776421, 0.9522197353910834, 0.07577917069975512, 0.07577917069975512, 0.03788958534987756, 0.7956812923474287, 0.22594534889271212, 0.2688936383516574, 0.35479021726954796, 0.10270243131486916, 0.04668292332494052, 0.8993774738190704, 0.8751542608851388, 0.018232380435107057, 0.09116190217553528, 0.018232380435107057, 0.1623942326530742, 0.811971163265371, 0.08624312848819224, 0.04704170644810486, 0.039201422040087384, 0.823229862841835, 0.3362897850009126, 0.133404212231767, 0.42244667206726216, 0.07781912380186408, 0.0305717986364466, 0.026220154925736715, 0.0043700258209561195, 0.2753116267202355, 0.6860940538901107, 0.008740051641912239, 0.27746068408395047, 0.021620313045502635, 0.5080773565693119, 0.0036033855075837725, 0.19097943190193994, 0.1485690195977645, 0.8171296077877047, 0.9372757257293043, 0.9412263334107991, 0.9208194115206176, 0.6465615541522829, 0.1403719163620088, 0.12761083305637164, 0.01701477774084955, 0.06380541652818582, 0.9686842577464351, 0.32348351463068803, 0.04528769204829632, 0.5758006560426248, 0.0323483514630688, 0.019409010877841283, 0.2038452854236358, 0.3298170910225119, 0.2679763864557909, 0.07787347982475974, 0.12139101266800784, 0.1684272422932589, 0.32241786381852416, 0.0962441384532908, 0.3994131745811568, 0.01924882769065816, 0.9208277060036767, 0.19538284857984728, 0.05009816630252495, 0.6212172621513093, 0.015029449890757484, 0.12023559912605987, 0.13347492388258989, 0.022245820647098312, 0.044491641294196624, 0.06673746194129494, 0.7563579020013427, 0.08725411228332937, 0.6297470712622902, 0.22003210923622188, 0.04931754172536008, 0.015174628223187716, 0.0345484700568066, 0.0345484700568066, 0.9328086915337781, 0.279472046735072, 0.17928395450929144, 0.19510312696599363, 0.3427487365618807, 0.005273057485567396, 0.9164212126079506, 0.9650275445792643, 0.970049861849209, 0.9681976390495003, 0.944261519512235, 0.5361972366205824, 0.14170926967829675, 0.20298895386350618, 0.08425956575466294, 0.03829980261575588, 0.38570463342202693, 0.08506768857428162, 0.3886046682597865, 0.06283408815145802, 0.07733426234025602, 0.9089067014167249, 0.01887425811341714, 0.9437129056708571, 0.01887425811341714, 0.9348995292870371, 0.1762805280208759, 0.23759549428900664, 0.4981841009285623, 0.08814026401043795, 0.0038321853917581716, 0.9656552012656717, 0.36656924485360615, 0.17737221525174493, 0.29325539588288496, 0.0827737004508143, 0.08040873758079103, 0.9583082732003139, 0.8739122840816927, 0.39359525320292305, 0.13312780623040044, 0.3901223539099561, 0.05556638868747149, 0.027783194343735745, 0.811349219938772, 0.17386054712973684, 0.9198488290888208, 0.9296648461045893, 0.6857087028240269, 0.1451500173861079, 0.1301344983461657, 0.01001034602662813, 0.025025865066570323, 0.309507998550857, 0.04203195042048675, 0.03056869121489945, 0.3782875537843807, 0.23690735691547077, 0.958262673659894, 0.9582263816030918, 0.9208983949603903, 0.9331011794299459, 0.14482182046390002, 0.062066494484528584, 0.1034441574742143, 0.1034441574742143, 0.5792872818556001, 0.9518454643754058, 0.93278242154048, 0.9860915527034396, 0.9732448333823972, 0.9600910785113694, 0.17756679357617805, 0.3044002175591624, 0.36781692955065454, 0.13740287598156636, 0.014797232798014839, 0.19865032962114132, 0.32471688495763484, 0.36482897074651915, 0.051572681728565536, 0.06112317834496656, 0.037947744158462436, 0.23527601378246712, 0.622343004198784, 0.09866413481200234, 0.25333006132978697, 0.2221829226416984, 0.26994186863010083, 0.19311225986614905, 0.06229427737617712, 0.9356638345399877, 0.06030307690669272, 0.9045461536003908, 0.8848432268977294, 0.5478446863627466, 0.15085578320133602, 0.08336766966389622, 0.15482567223295013, 0.0674881135374398, 0.04162625933940338, 0.9157777054668744, 0.8874332225173389, 0.08067574750157626, 0.03192592453634168, 0.12770369814536672, 0.7342962643358586, 0.09577777360902504, 0.9694336432919564, 0.1594134607780646, 0.09726923030525976, 0.39177884428507403, 0.14590384545788965, 0.2080480759306945, 0.3852695074301039, 0.18575494108237153, 0.21602611666616542, 0.0853096766452373, 0.12796451496785594, 0.9751495126055535, 0.020747861970330926, 0.17486586832085163, 0.3818172170675476, 0.30962488610939787, 0.08663079714977971, 0.04652394661747429, 0.9829392799140202, 0.033836833550345086, 0.9474313394096624, 0.1397696158057493, 0.4669398338091513, 0.30764932194114647, 0.06480937492668822, 0.021082567747235926, 0.05169569869202521, 0.22401469433210927, 0.03446379912801681, 0.6892759825603362, 0.9649867597265513, 0.9149275225498731, 0.8676838870058712, 0.017707834428691248, 0.035415668857382496, 0.07083133771476499, 0.24763088508010636, 0.2918506859872682, 0.3133288749993182, 0.07833221874982955, 0.06948825856839719, 0.18949133489413908, 0.6939812524694444, 0.06644501353430851, 0.044296675689539, 0.004921852854393223, 0.051169664297928574, 0.9210539573627143, 0.26641786834223186, 0.21748397415692394, 0.17398717932553914, 0.2881662657579242, 0.054370993539230986, 0.9434995086974685, 0.9687191206273252, 0.9684173873750083, 0.948469549807759, 0.9401281113380651, 0.9458784562482045, 0.9735704842894894, 0.9341087203380434, 0.06406091583118702, 0.012812183166237402, 0.5701421508975644, 0.3203045791559351, 0.03203045791559351, 0.587229670179663, 0.05385075578066778, 0.06410804259603307, 0.11026583326517689, 0.18463116267657526, 0.9441875697066777, 0.9645152573781361, 0.9353654328580854, 0.965825586876682, 0.964736618194659, 0.3378420516135592, 0.18769002867419957, 0.21897170011989947, 0.05317884145768988, 0.20020269725247952, 0.9343891652730122, 0.4235313036524206, 0.10529783792463496, 0.30185380205062023, 0.07019855861642331, 0.09827798206299262, 0.972891159763858, 0.9480479609281924, 0.9611849001597679, 0.9766819299902059, 0.9410645123983261, 0.9221328849135102, 0.02616395755437085, 0.9419024719573506, 0.02616395755437085, 0.02182708676908497, 0.8294292972252288, 0.08730834707633987, 0.04365417353816994, 0.12961728358947694, 0.5230171092206964, 0.14553519560923725, 0.15690513276620893, 0.04320576119649231, 0.21524163570014268, 0.1034815556250686, 0.6581426937754363, 0.02069631112501372, 0.05768814620651632, 0.9230103393042611, 0.24067369213603396, 0.35024870644187056, 0.29350450260491945, 0.06065759720501669, 0.052830810468885506, 0.6849481347522599, 0.09574543819117612, 0.14730067414027095, 0.07365033707013548, 0.20082217387108922, 0.24890635634726552, 0.31773273518571393, 0.15179516428753692, 0.0801403041269605, 0.02186041567890681, 0.8306957957984588, 0.08744166271562724, 0.04372083135781362, 0.23725842943308106, 0.2935570398070325, 0.29556770446324504, 0.13069320265381584, 0.044234622436676134, 0.0945948671168785, 0.34864965308792367, 0.42702825727048016, 0.10810841956214687, 0.021621683912429374, 0.1915817193268315, 0.5517553516612747, 0.206908256872978, 0.043425189714081804, 0.005108845848715507, 0.4668261866113405, 0.18375073302786807, 0.09435848452782414, 0.03476365219446152, 0.2234806212501098, 0.43503519514019195, 0.1924194132350849, 0.016732122890007382, 0.033464245780014765, 0.31791033491014026, 0.1779447456019271, 0.4671049572050587, 0.274331482802971, 0.04201473160045501, 0.03707182200040148, 0.9504039147187303, 0.05798760926964465, 0.5143248822177178, 0.36053165763300804, 0.05546640886661662, 0.010084801612112114, 0.9618623771467858, 0.9442073933107851, 0.9812248933014569, 0.03902491778108818, 0.494315625227117, 0.4422824015189994, 0.013008305927029394, 0.964436177976532, 0.026789893832681443, 0.1362121313080571, 0.5086671778535258, 0.22560134247896957, 0.06810606565402855, 0.059592807447274986, 0.9433068997083588, 0.8356503203232106, 0.028815528287007265, 0.1296698772915327, 0.9719586228467725, 0.9735531752115686, 0.32344685110833227, 0.17497944404221252, 0.434797406407922, 0.026512036976092807, 0.04241925916174849, 0.17481966483393585, 0.2892470818161484, 0.46724528601070126, 0.025428314884936125, 0.04449955104863822, 0.972945310050162, 0.9644863564568825, 0.01819785578220533, 0.058943086908681844, 0.058943086908681844, 0.8841463036302276, 0.96606880923497, 0.9143755961904337, 0.21352312168234977, 0.47249493994768965, 0.1672169025223221, 0.11919563820821935, 0.02744072246520158, 0.9510853145901189, 0.9518460836794989, 0.6548075934013731, 0.03604445468264439, 0.18022227341322195, 0.030037045568870324, 0.1021259549341591, 0.9806970951715847, 0.5049584357581677, 0.0947174126810422, 0.2859621249096433, 0.027148302997750947, 0.08747786521497528, 0.005894719749140528, 0.32420958620272905, 0.6248402934088959, 0.041263038243983696, 0.34042914946278713, 0.05673819157713118, 0.018912730525710394, 0.576838281034167, 0.944167665533994, 0.9696841101348072, 0.9343860852176008, 0.5845583357087514, 0.09108700094091846, 0.1251195067869759, 0.06806501169211489, 0.13212619916704654, 0.5939575591202777, 0.0679363221216004, 0.23292453298834423, 0.0368797177231545, 0.0679363221216004, 0.597498988832741, 0.04336686209269894, 0.038548321860176836, 0.0048185402325221045, 0.3083865748814147, 0.45498353166595196, 0.26466362299522694, 0.1635561715139043, 0.0832649600434422, 0.03271123430278086, 0.9356415587710611, 0.15040674446809826, 0.16789590080159805, 0.1294197568678985, 0.21336770726869753, 0.3357918016031961, 0.8989678837433954, 0.18161694661406483, 0.03632338932281297, 0.2179403359368778, 0.5448508398421945, 0.018161694661406484, 0.9694803149515214, 0.8840475278453066, 0.2806442120618756, 0.33285708872455017, 0.24409519839800348, 0.08484592457684613, 0.05873948624550886, 0.2307967631239286, 0.008876798581689561, 0.008876798581689561, 0.017753597163379122, 0.7367742822802336, 0.9596781091997834, 0.9579026089190747, 0.9042330186308258, 0.9601859774201201, 0.9475344692427499, 0.10810153812530222, 0.28955769140705956, 0.5173430753139464, 0.07721538437521587, 0.0077215384375215875, 0.13458071142028782, 0.47221302252732567, 0.2526339670521192, 0.12041432074446805, 0.018888520901093026, 0.24864274711892206, 0.1255053866409797, 0.4783412849335453, 0.11840130815186764, 0.028416313956448235, 0.32908669360836607, 0.27804467582421133, 0.28879036377877026, 0.05775807275575405, 0.047012384801195156, 0.9521394458779818, 0.9709262902454189, 0.9706092767636078, 0.8987400168805473, 0.9209934187028129, 0.08149381964285544, 0.04074690982142772, 0.8556851062499822, 0.10164008845249445, 0.8741047606914522, 0.9370111063968894, 0.9433811907877552, 0.15837831954281692, 0.08798795530156496, 0.03519518212062599, 0.7215012334728327, 0.23147595001741617, 0.3899479465678011, 0.21367010770838415, 0.11217680654690168, 0.05341752692709604, 0.049454620290105666, 0.890183165221902, 0.0050424514235279634, 0.9883204790114808, 0.0033616342823519756, 0.0016808171411759878, 0.0016808171411759878, 0.922887750302325, 0.9154185110205413, 0.9341301809720841, 0.9736096070340762, 0.21446709099320813, 0.21446709099320813, 0.10212718618724197, 0.45957233784258883, 0.010212718618724196, 0.9619308985201558, 0.21611995686322918, 0.11355455360610346, 0.2820548589570957, 0.24542435779383653, 0.1428589545367108, 0.8018016573138297, 0.039432868392483424, 0.14458718410577256, 0.013144289464161142, 0.19608295333813466, 0.04456430757684879, 0.11586719969980684, 0.017825723030739513, 0.623900306075883, 0.46182468968293866, 0.04948121675174343, 0.4090447251477457, 0.009896243350348687, 0.0692737034524408, 0.2747168603876613, 0.21366866919040325, 0.35814938835724736, 0.07122288973013442, 0.08343252796958603, 0.1493814462725253, 0.44513665869128344, 0.25966305090324865, 0.11128416467282086, 0.034087041431314496, 0.25407028507820173, 0.2036761789469882, 0.46824523613585944, 0.02099754422133899, 0.05249386055334747, 0.012758340340121155, 0.8420504624479962, 0.12758340340121155, 0.06160818104366156, 0.06160818104366156, 0.03080409052183078, 0.831710444089431, 0.9719565398337577, 0.9702066892797627, 0.03217027062890559, 0.9168527129238092, 0.03217027062890559, 0.016085135314452794, 0.23385387899875582, 0.3547221760093487, 0.14977158542616945, 0.23385387899875582, 0.031530860089719885, 0.9607110717846113, 0.02689934232263348, 0.8607789543242713, 0.10759736929053391, 0.977406850419251, 0.8739294811226559, 0.9735303500935016, 0.9473973527516584, 0.9697985343234793, 0.05607298602647319, 0.809053084096256, 0.0720938391768941, 0.05607298602647319, 0.02203034179434814, 0.19827307614913328, 0.1101517089717407, 0.5948192284473998, 0.06609102538304443, 0.31557133282478234, 0.13108347671183268, 0.19905268685870886, 0.058259322983036745, 0.2912966149151837, 0.25414228940282674, 0.3366270675423407, 0.27755013184782396, 0.078026141483324, 0.0546182990383268, 0.014614835679049722, 0.9645791548172817, 0.007307417839524861, 0.014614835679049722, 0.0430209477243029, 0.0860418954486058, 0.02151047386215145, 0.860418954486058, 0.9515786050476901, 0.9042393238823178, 0.23012388942393236, 0.37600599789803235, 0.3636779323931788, 0.020546775841422532, 0.010273387920711266, 0.22199627366338637, 0.04439925473267727, 0.01479975157755909, 0.7251878273003954, 0.9267668106407585, 0.2647841035982805, 0.31728440000138786, 0.2533709956845615, 0.12554418705090886, 0.03880456690664456, 0.8139554615000634, 0.02051988558403521, 0.08891950419748591, 0.07523958047479577, 0.9726122321493291, 0.955155815440584, 0.1608102100416798, 0.2132483220117928, 0.5523481127518568, 0.034958741313408655, 0.03845461544474952, 0.22856229917166682, 0.2688021405751293, 0.3154803566031458, 0.13922985125598014, 0.047483012856085714, 0.9751915715050176, 0.9410721580304885, 0.3083028590464832, 0.2968842346373542, 0.2501716802363719, 0.07577814380603795, 0.06954980321924031, 0.9544063895189208, 0.19012900150451723, 0.4868454735494456, 0.17860603171636466, 0.09506450075225861, 0.048972621599648374, 0.9347782057246367, 0.9412160585819878, 0.15905107203746366, 0.7952553601873182, 0.960311695440516, 0.14897927822292356, 0.3022767963943377, 0.4296648748748086, 0.10795599871226345, 0.008636479896981077, 0.07653441327920227, 0.5879234474629629, 0.19481487016524213, 0.1148016199188034, 0.024351858770655266, 0.952731192938086, 0.021171804287513023, 0.8845439708272416, 0.9573104650799222, 0.02279310631142672, 0.01139655315571336, 0.47907976493754584, 0.4094176562604027, 0.08188353125208053, 0.006110711287468697, 0.023220702892381047, 0.9804884121114688, 0.9460893665226244, 0.1812206540132378, 0.4481801120757494, 0.20655257339143232, 0.037023574475822774, 0.12665959689097267, 0.4781504009194236, 0.06315193974407482, 0.4510852838862487, 0.9440881679766799, 0.9762674614447029, 0.023567642659894757, 0.023567642659894757, 0.9427057063957902, 0.946228106282216, 0.9357706513581698, 0.4170011992842482, 0.30661852888547664, 0.17415932440695073, 0.024529482310838128, 0.0760413951635982, 0.9511903301165124, 0.38330603865842205, 0.3308881188418857, 0.1856467993502329, 0.03822139986622442, 0.06224627978213691, 0.22725801448460828, 0.011960948130768857, 0.7056959397153626, 0.047843792523075426, 0.9481117117803264, 0.3384204678881796, 0.1814992453478505, 0.3006081251073774, 0.1417962854280082, 0.03781234278080219, 0.11491089053128745, 0.7916083569933136, 0.038303630177095815, 0.051071506902794425, 0.954545117693928, 0.9638534155443481, 0.9431880309238959, 0.9039654969402635, 0.671639046291338, 0.10933658893114805, 0.09371707622669832, 0.020826016939266292, 0.10413008469633148, 0.20643058352675445, 0.6045467088997809, 0.12680735845214916, 0.04128611670535089, 0.020643058352675447, 0.30062628319714996, 0.29748384818115187, 0.2959126306731528, 0.04870774274797029, 0.057087569457298516, 0.11416236655275569, 0.3272654507845663, 0.1446056643001572, 0.015221648873700758, 0.39576287071621974, 0.9795327144584381, 0.21171548162847725, 0.40714515697784087, 0.28870292949337806, 0.062182169429342966, 0.028130029027559914, 0.4297669356812679, 0.2445225668531352, 0.21488346784063395, 0.09632707179062902, 0.01296710581796929, 0.9632263913672475, 0.018523584449370144, 0.16638675538207795, 0.3105886100465455, 0.3571769015535273, 0.07986564258339741, 0.08430262272691949, 0.91170064226225, 0.07013081863555769, 0.9351423445317395, 0.049455607901844584, 0.8902009422332025, 0.06532734922434899, 0.032663674612174495, 0.8819192145287114, 0.9043652143649267, 0.5079515516455044, 0.009406510215657488, 0.02821953064697247, 0.009406510215657488, 0.44210598013590197, 0.9459002713683061, 0.39679704883318234, 0.09533435588849186, 0.3529947772087401, 0.04895548005084717, 0.10306416852809931, 0.9317386916050406, 0.05273992593990796], \"Term\": [\"action\", \"action\", \"action\", \"action\", \"action\", \"activity\", \"activity\", \"activity\", \"activity\", \"actor\", \"adaboost\", \"adaptation\", \"adaptation\", \"adaptation\", \"adaptation\", \"admm\", \"agd\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"also\", \"also\", \"also\", \"also\", \"also\", \"animal\", \"animal\", \"animal\", \"animal\", \"antagonist\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"arousal\", \"artifact\", \"artifact\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"ax\", \"ax\", \"ax\", \"base\", \"base\", \"base\", \"base\", \"base\", \"basis_vector\", \"basis_vector\", \"batch_normalization\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"beta\", \"beta\", \"bias\", \"bias\", \"bias\", \"bias\", \"bias\", \"birth\", \"birth\", \"boost\", \"boost\", \"boost\", \"boost\", \"brain_state\", \"brownboost\", \"buyer\", \"calibrate\", \"calibrate\", \"calibrated_classifier\", \"calibration\", \"cap\", \"car\", \"car\", \"car\", \"car\", \"case\", \"case\", \"case\", \"case\", \"case\", \"cf_ht\", \"character\", \"character\", \"character\", \"character\", \"chip\", \"chip\", \"circle\", \"circle\", \"circle\", \"circle\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cnn\", \"cnn\", \"cnn_message\", \"coagent\", \"codebook\", \"component\", \"component\", \"component\", \"component\", \"component\", \"concentrator\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"contact\", \"context\", \"context\", \"context\", \"context\", \"context\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"conversion\", \"conversion\", \"conversion\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost_go\", \"count_mixture\", \"critic\", \"dag\", \"daquar\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"ddp\", \"decision_tree\", \"decision_tree\", \"decision_tree\", \"degree_separability\", \"denote\", \"denote\", \"denote\", \"denote\", \"denote\", \"density_functional\", \"different\", \"different\", \"different\", \"different\", \"different\", \"disinhibition\", \"disparate\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"dot_product\", \"dot_product\", \"dpf\", \"dpps\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge_filtere\", \"education\", \"ego_network\", \"eigenflow\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"elbo\", \"eld\", \"engine\", \"equalized_odd\", \"erm\", \"error\", \"error\", \"error\", \"error\", \"error\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"example\", \"example\", \"example\", \"example\", \"example\", \"excitation\", \"facebook\", \"facebook\", \"facebook_google\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"fairness\", \"fairness\", \"false_negative\", \"false_negative\", \"false_positive\", \"false_positive\", \"false_positive\", \"false_positive\", \"fdkm\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"firing_rate\", \"firing_rate\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"food\", \"friend\", \"friend\", \"function\", \"function\", \"function\", \"function\", \"function\", \"game\", \"game\", \"game\", \"game\", \"gamma_nb\", \"generalized_fp\", \"generative\", \"generative\", \"generative\", \"generative\", \"give\", \"give\", \"give\", \"give\", \"give\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"graph_matche\", \"graph_matche\", \"group\", \"group\", \"group\", \"group\", \"group\", \"gru\", \"gy\", \"habituation\", \"halflife\", \"halflive\", \"handicapped\", \"hilbert_space\", \"hyperkernel\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"image\", \"image\", \"image\", \"image\", \"image\", \"img_bow\", \"inactive_node\", \"inertia\", \"infant\", \"influence_maximization\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information_theoretic\", \"input\", \"input\", \"input\", \"input\", \"input\", \"insect\", \"interconnect\", \"intervention\", \"interventional\", \"inverse_covariance\", \"invisible\", \"isotonic_regression\", \"isotonic_regression\", \"isotonic_regression\", \"iterate\", \"iterate\", \"iterate\", \"iterate\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"knowledge_partitioning\", \"knowledge_partitioning\", \"large\", \"large\", \"large\", \"large\", \"large\", \"latent\", \"latent\", \"latent\", \"latent\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learner\", \"learner\", \"learner\", \"learner\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"let\", \"let\", \"let\", \"let\", \"let\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"local\", \"local\", \"local\", \"local\", \"local\", \"location\", \"location\", \"location\", \"location\", \"location\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log_det\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"lpboost\", \"lstm\", \"macro_action\", \"margin\", \"margin\", \"margin\", \"margin\", \"markov_decision\", \"markov_decision\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"mc\", \"mcmc\", \"mcmc\", \"mcmc\", \"mdp\", \"mdps\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"memoize\", \"merge\", \"merge\", \"message\", \"message\", \"message\", \"message_estimator\", \"meta\", \"method\", \"method\", \"method\", \"method\", \"method\", \"mhmm\", \"minibatch\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mlfre\", \"model\", \"model\", \"model\", \"model\", \"model\", \"module\", \"module\", \"module\", \"module\", \"motion\", \"motion\", \"motion\", \"motion\", \"nation\", \"nb\", \"nb_process\", \"network\", \"network\", \"network\", \"network\", \"network\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"new\", \"new\", \"new\", \"new\", \"new\", \"newton_direction\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node_gij\", \"normalization\", \"normalization\", \"normalization\", \"normalization\", \"normalization\", \"novelty\", \"nsdbn\", \"number\", \"number\", \"number\", \"number\", \"number\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object_perception\", \"objects_tend\", \"occluder\", \"occlusion_event\", \"ofo\", \"online\", \"online\", \"online\", \"online\", \"online\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"output\", \"output\", \"output\", \"output\", \"output\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parity\", \"patch\", \"pauc\", \"pd_pd\", \"pe\", \"perception\", \"perception\", \"perception\", \"perceptron\", \"perceptron\", \"pf\", \"pile\", \"pixel\", \"pixel\", \"pixel\", \"pixel\", \"point\", \"point\", \"point\", \"point\", \"point\", \"pole\", \"pole\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"polyhedron\", \"pomdp\", \"ponss\", \"poss\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"prec\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"price\", \"price\", \"price\", \"price\", \"principle\", \"principle\", \"principle\", \"principle\", \"principle\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"process\", \"process\", \"process\", \"process\", \"process\", \"processor\", \"processor\", \"processor\", \"profile\", \"profile\", \"profile\", \"profile\", \"protagonist\", \"protocol\", \"query\", \"query\", \"query\", \"query\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rddp\", \"read\", \"read\", \"read\", \"reading_mechanism\", \"recidivism\", \"refit\", \"reflex\", \"regularized_risk\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"relaxation\", \"relaxation\", \"relaxation\", \"relaxation\", \"relaxation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"result\", \"result\", \"result\", \"result\", \"result\", \"reward\", \"reward\", \"reward\", \"reward\", \"roc_curve\", \"roc_curve\", \"roc_curve\", \"roc_curve\", \"rocch\", \"row_parallel\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"scene\", \"scene\", \"scene\", \"scene\", \"sdp\", \"see\", \"see\", \"see\", \"see\", \"see\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"seller\", \"separator\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"set\", \"set\", \"set\", \"set\", \"set\", \"setpoint\", \"short_dot\", \"show\", \"show\", \"show\", \"show\", \"show\", \"sinusoid\", \"size\", \"size\", \"size\", \"size\", \"size\", \"sleep\", \"sn\", \"soft_margin\", \"soft_margin\", \"softboost\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solve\", \"solve\", \"solve\", \"solve\", \"solve\", \"sparseness\", \"sparseness\", \"spelke\", \"spike\", \"spike\", \"spike\", \"state\", \"state\", \"state\", \"state\", \"state\", \"stationary_ergodic\", \"statistical_accuracy\", \"step\", \"step\", \"step\", \"step\", \"step\", \"stimulus\", \"stimulus\", \"stimulus\", \"strong_annotation\", \"subgraph\", \"subset_selection\", \"subset_selection\", \"subset_selection\", \"surrogate\", \"svrg\", \"system\", \"system\", \"system\", \"system\", \"system\", \"thalnet\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time_serie\", \"time_serie\", \"time_serie\", \"time_serie\", \"timestep\", \"training\", \"training\", \"training\", \"training\", \"training\", \"trial\", \"trial\", \"trial\", \"trial\", \"triangle_inequality\", \"triangulate\", \"twitter\", \"uniform_newscast\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"update\", \"update\", \"update\", \"update\", \"update\", \"use\", \"use\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"user\", \"user\", \"valuation\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variational_inference\", \"variational_inference\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vehicle\", \"vehicle\", \"velocity_eld\", \"vibration\", \"vibration\", \"visible\", \"visible\", \"visible\", \"visible_invisible\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"voice\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight_normalization\", \"weight_normalization\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 4, 1, 3, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1352423223027202565462810894\", ldavis_el1352423223027202565462810894_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1352423223027202565462810894\", ldavis_el1352423223027202565462810894_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1352423223027202565462810894\", ldavis_el1352423223027202565462810894_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1      0.025704 -0.040730       1        1  28.559974\n",
       "3     -0.061932 -0.036690       2        1  27.450565\n",
       "0     -0.039690 -0.037846       3        1  26.770950\n",
       "2     -0.054984  0.095843       4        1   8.916237\n",
       "4      0.130901  0.019422       5        1   8.302275, topic_info=            Term         Freq        Total Category  logprob  loglift\n",
       "2885      policy   594.000000   594.000000  Default  30.0000  30.0000\n",
       "2065      object   225.000000   225.000000  Default  29.0000  29.0000\n",
       "54    classifier   228.000000   228.000000  Default  28.0000  28.0000\n",
       "294      network   999.000000   999.000000  Default  27.0000  27.0000\n",
       "285        model  1657.000000  1657.000000  Default  26.0000  26.0000\n",
       "...          ...          ...          ...      ...      ...      ...\n",
       "73      consider    53.497965   436.605634   Topic5  -5.6636   0.3893\n",
       "480         time    56.532931   915.717376   Topic5  -5.6084  -0.2962\n",
       "429          set    59.405609  1242.549629   Topic5  -5.5588  -0.5519\n",
       "182         give    54.645976   791.500624   Topic5  -5.6423  -0.1844\n",
       "409       result    48.793019   897.135225   Topic5  -5.7556  -0.4230\n",
       "\n",
       "[413 rows x 6 columns], token_table=      Topic      Freq                  Term\n",
       "term                                       \n",
       "2725      1  0.041623                action\n",
       "2725      2  0.665974                action\n",
       "2725      3  0.041623                action\n",
       "2725      4  0.182102                action\n",
       "2725      5  0.067638                action\n",
       "...     ...       ...                   ...\n",
       "511       3  0.352995                weight\n",
       "511       4  0.048955                weight\n",
       "511       5  0.103064                weight\n",
       "1197      1  0.931739  weight_normalization\n",
       "1197      3  0.052740  weight_normalization\n",
       "\n",
       "[874 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 4, 1, 3, 5])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b5bd2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model = lda_model, texts = clean_words_lemmatized, dictionary = id2word, coherence = 'c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7abcce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_lda = coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "47afbb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model Coherence Score is :  0.2759346771765422\n"
     ]
    }
   ],
   "source": [
    "print(\"The model Coherence Score is : \", coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93617406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
